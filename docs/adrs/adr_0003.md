---
title: "Feature Tier System"
description: "Three-tier feature architecture allowing graceful degradation from LLM-powered features to core functionality with zero external dependencies."
type: adr
category: architecture
tags:
  - feature-flags
  - graceful-degradation
  - dependency-management
  - configuration
  - tiered-features
status: accepted
created: 2025-12-28
updated: 2026-01-04
author: Claude (Architect)
project: subcog
technologies:
  - llm
  - mcp
audience:
  - developers
  - architects
confidence: high
completeness: complete
---

# ADR-0003: Feature Tier System

## Status

Accepted

**Note:** Tier 1 persistence has been updated per [ADR-0047](adr_0047.md) - SQLite replaces Git Notes as the primary persistence layer.

## Context

### Background and Problem Statement

Subcog provides a range of features spanning from basic memory capture and retrieval to sophisticated LLM-powered analysis and consolidation. These features have fundamentally different dependency requirements: some can operate with zero external dependencies (embedded database, local computation), others require local machine learning models (entity extraction, embedding generation), and still others require access to external LLM APIs (GPT-4, Claude) for advanced reasoning capabilities.

The architectural challenge is how to organize these features such that:
1. Users can run Subcog with minimal dependencies for basic functionality
2. Advanced features can be enabled incrementally as dependencies are satisfied
3. The system degrades gracefully when optional dependencies are unavailable
4. Users understand what capabilities they have access to based on their configuration

### The Dependency Spectrum

Subcog features span a spectrum of external dependency requirements:

**Zero External Dependencies (Fully Embedded)**:
- SQLite database for persistence (bundled via rusqlite)
- Full-text search via FTS5 (bundled with SQLite)
- Basic MCP server for AI agent integration
- CLI commands for manual interaction

**Local Processing Dependencies (No Network Required)**:
- Embedding generation via fastembed (ONNX Runtime, local model files)
- Vector similarity search via usearch (HNSW algorithm, in-memory)
- Entity extraction via regex patterns (built-in)
- Temporal extraction via chrono parsing (built-in)
- Secret/PII filtering via pattern matching (built-in)

**External Service Dependencies (Network Required)**:
- LLM-powered implicit capture (requires OpenAI/Anthropic API)
- Memory consolidation with semantic understanding (requires LLM)
- Supersession detection (requires LLM to understand semantic relationships)
- Query expansion with synonyms (requires LLM)
- Summarization of long memories (requires LLM)

### The Problem with Flat Feature Flags

A flat feature flag system (where each feature is independently toggled) creates several problems:

1. **Combinatorial Explosion**: With N features, there are 2^N possible configurations. Users cannot reason about which combinations are valid or recommended.

2. **Hidden Dependencies**: Features may have dependencies on other features that are not obvious. Enabling LLM consolidation without enabling embeddings would produce poor results.

3. **Configuration Burden**: Users must understand each feature individually and make N separate decisions, even when features naturally group together.

4. **Unclear Capability Levels**: Users cannot easily communicate their system's capabilities ("I have Tier 2 enabled" vs. listing 15 individual features).

5. **Documentation Complexity**: Each feature requires documentation, and the documentation must explain interactions between features.

### The Case for Tiered Features

A tiered system addresses these problems by grouping features into coherent capability levels:

1. **Clear Progression**: Users understand that Tier 2 includes all of Tier 1, and Tier 3 includes all of Tier 2.

2. **Dependency Clarity**: Each tier has clear external dependency requirements.

3. **Simple Configuration**: Users enable a tier level rather than individual features.

4. **Predictable Degradation**: When a tier's dependencies are unavailable, the system falls back to the previous tier.

5. **Easy Communication**: "Subcog Tier 2" conveys a specific capability level.

### Tier Classification Criteria

Features are assigned to tiers based on their dependency characteristics:

| Criterion | Tier 1 (Core) | Tier 2 (Enhanced) | Tier 3 (LLM-Powered) |
|-----------|---------------|-------------------|----------------------|
| **External Dependencies** | None - all functionality embedded in binary | Local only - models/files on disk, no network | External API required - network calls to LLM providers |
| **Network Requirement** | Not required - fully offline capable | Not required - fully offline capable | Required - must reach LLM provider endpoints |
| **Latency Budget** | Less than 10ms - must not block AI assistant workflows | Less than 100ms - acceptable for background processing | Less than 5 seconds - acceptable for explicit user actions |
| **Failure Mode** | Must not fail - core functionality always available | Graceful fallback to Tier 1 - no data loss | Graceful fallback to Tier 2 - reduced functionality only |
| **Data Sensitivity** | All data stays local - no external transmission | All data stays local - models run in-process | Data sent to LLM provider - subject to provider's privacy policy |
| **Cost Model** | Zero cost - no usage-based charges | Zero cost - local computation only | Per-token cost - LLM API calls incur charges |
| **Availability Guarantee** | 100% - no external dependencies to fail | 100% - no external dependencies to fail | Subject to LLM provider availability and rate limits |

## Decision Drivers

### Primary Decision Drivers

The following factors were weighted most heavily in the feature organization decision:

1. **Zero-Dependency Core Requirement**: The fundamental use case of capturing and searching memories must work without any external dependencies. A user should be able to download the Subcog binary and immediately start capturing memories without configuring LLM providers, downloading models, or setting up accounts.

2. **Privacy-Conscious Defaults Requirement**: The default configuration must not send any data to external services. Users must explicitly opt-in to features that transmit data (Tier 3 LLM features), and the system must clearly communicate when data will be sent externally.

3. **Graceful Degradation Requirement**: When optional dependencies are unavailable (LLM API down, model file missing, network unreachable), the system must continue functioning at a lower tier rather than failing entirely. Users should always be able to capture memories.

4. **Explicit Opt-In Requirement**: Advanced features, especially those with cost implications (LLM API calls) or privacy implications (external data transmission), must require explicit user opt-in. Features should not be enabled by default just because dependencies are available.

5. **Clear Documentation Requirement**: Users must be able to understand what capabilities they have based on their configuration. The tier system provides a simple mental model: "I have Tier 2, so I can do X, Y, Z but not A, B, C."

### Secondary Decision Drivers

The following factors influenced the decision but were not individually decisive:

1. **Testing Isolation**: Tier-based organization allows testing each tier independently. Tier 1 tests run without mocking LLM APIs, Tier 2 tests can mock embedding services, and Tier 3 tests mock LLM providers.

2. **Deployment Profiles**: Common deployment scenarios map to tiers:
   - CI/CD pipelines: Tier 1 (no external dependencies)
   - Developer workstations: Tier 2 (local models for semantic search)
   - Production systems: Tier 3 (full LLM integration)

3. **Resource Planning**: Each tier has predictable resource requirements:
   - Tier 1: Minimal (SQLite, text processing)
   - Tier 2: Moderate (embedding model ~500MB, vector index memory)
   - Tier 3: Variable (depends on LLM usage patterns)

## Considered Options

### Option 1: Three-Tier Feature Architecture (Selected)

**Description**: Organize features into three hierarchical tiers based on their external dependency requirements. Each tier includes all features from lower tiers plus additional capabilities.

**Tier Definitions**:

**Tier 1: Core (Always Available, Zero External Dependencies)**

Tier 1 provides the fundamental memory capture and retrieval functionality that must always work:

- **Memory Capture and Storage**: Persist memories to SQLite database with full ACID guarantees. Memories include content, metadata (namespace, domain, timestamps), and relationships.

- **Keyword Search via BM25**: Full-text search using SQLite FTS5 extension. Supports tokenization, stemming-like behavior, and relevance ranking. Enables finding memories by keyword even without semantic understanding.

- **Multi-Domain Memories**: Support for project, user, and global memory scopes. Project memories are tied to a specific repository, user memories span all projects for a user, and global memories are shared organization-wide.

- **MCP Tools for AI Agent Integration**: Model Context Protocol server exposing memory operations as tools. AI assistants can capture, search, and retrieve memories through standardized tool calls.

- **Basic CLI Operations**: Command-line interface for manual memory management including capture, search, list, show, archive, and delete operations.

- **Memory Lifecycle Management**: Archive and delete operations with audit trail. Memories can be marked as archived (soft delete) or permanently removed.

**Tier 2: Enhanced (Opt-In, Local Processing Only)**

Tier 2 adds capabilities that require local processing resources but no external network access:

- **Entity Extraction via Regex and Local NER**: Automatic identification of entities (people, organizations, technologies, concepts) in captured memories. Uses regex patterns for structured entities (emails, URLs, file paths) and optional local NER models for natural language entities.

- **Temporal Extraction via Date/Time Parsing**: Identification and normalization of temporal references in memories. Converts phrases like "last Tuesday" or "Q3 2025" into structured timestamps for temporal queries.

- **Secrets Filtering via Pattern Detection**: Automatic detection and redaction of secrets (API keys, passwords, tokens) before storage. Uses regex patterns for common secret formats and entropy analysis for unstructured secrets.

- **PII Filtering via Pattern Detection**: Automatic detection and optional redaction of personally identifiable information (emails, phone numbers, SSNs). Configurable to redact, flag, or allow PII.

- **Hook System for Pre/Post Capture Processing**: Extensible hooks that run before and after memory capture. Enables custom processing like classification, enrichment, or validation.

- **Semantic Search via Embeddings**: Vector similarity search using locally-generated embeddings (fastembed with MiniLM-L6-v2). Enables finding semantically similar memories even when keywords differ.

- **Advanced Observability**: Prometheus metrics export, OpenTelemetry tracing, and structured JSON logging. Enables monitoring Subcog in production deployments.

- **Webhook Notifications**: HTTP webhook calls when memory events occur (capture, archive, delete). Enables integration with external systems like Slack, email, or custom dashboards.

**Tier 3: LLM-Powered (Opt-In, Requires LLM Provider)**

Tier 3 adds capabilities that require access to external LLM APIs for advanced reasoning:

- **Implicit Capture from Conversations**: Automatic detection of decision-worthy content in AI assistant conversations. The LLM analyzes conversation context to identify architectural decisions, learnings, and patterns without explicit user action.

- **Memory Consolidation**: Intelligent merging of related memories using LLM understanding. When multiple memories cover overlapping topics, the LLM can consolidate them into a single, comprehensive memory while preserving attribution.

- **Supersession Detection**: Identification of memories that have been superseded by newer information. The LLM understands semantic relationships to detect when a newer memory contradicts or updates an older one.

- **Temporal Reasoning**: Understanding of complex temporal references like "before we switched to Rust" or "during the Q2 refactor." The LLM grounds these references against known project milestones.

- **Query Expansion with Synonyms**: Enhancement of search queries with semantically related terms. When searching for "database," the LLM might expand to include "PostgreSQL," "SQLite," "persistence," and "storage."

- **Summarization of Long Memories**: Condensation of verbose memories into concise summaries while preserving key information. Useful for generating context windows for AI assistants.

**Advantages**:
- Clear mental model for users (three capability levels)
- Predictable degradation path (Tier 3 -> Tier 2 -> Tier 1)
- Simple configuration (enable tier level)
- Privacy-conscious defaults (Tier 1 sends no data externally)
- Testable in isolation (each tier can be tested independently)

**Disadvantages**:
- Some feature combinations don't fit neatly into tiers
- Users wanting specific features must enable entire tier
- Tier boundaries may need adjustment as features evolve

**Risk Assessment**:
- **Technical Risk**: Low. Feature flags are well-understood patterns.
- **Usability Risk**: Low. Tiered systems are intuitive for users.
- **Flexibility Risk**: Medium. Some users may want non-tier feature combinations.

### Option 2: Flat Feature Flags

**Description**: Each feature is independently toggleable via configuration flags with no hierarchical organization.

**Advantages**:
- Maximum flexibility for feature combinations
- No artificial groupings
- Each feature documented independently

**Disadvantages**:
- 2^N configuration combinations to reason about
- Hidden dependencies between features
- Users must understand each feature individually
- No clear capability levels for communication
- Configuration burden increases with feature count

**Disqualifying Factor**: The configuration complexity for users is unacceptable. With 15+ features, users cannot reasonably understand all combinations. The tier system provides a simpler mental model while covering the most common use cases.

### Option 3: Two-Tier System (Core + Everything Else)

**Description**: Simplify to just two tiers: core (offline) and enhanced (requires external dependencies).

**Advantages**:
- Simpler than three tiers
- Clear offline vs. online distinction

**Disadvantages**:
- Conflates local processing (embeddings) with external APIs (LLM)
- No distinction between "costs nothing" (local models) and "costs money" (LLM API)
- No privacy distinction between "data stays local" and "data sent externally"
- Users wanting semantic search would need to configure LLM provider even though embeddings are local

**Disqualifying Factor**: The two-tier system fails to capture the important distinction between local processing (Tier 2) and external API calls (Tier 3). Users should be able to get semantic search without configuring an LLM provider or sending data externally.

### Option 4: Capability-Based System

**Description**: Organize features by capability (search, analysis, integration) rather than dependency requirements.

**Advantages**:
- Features grouped by function
- Intuitive for feature discovery

**Disadvantages**:
- Doesn't address dependency requirements
- Users still must understand which capabilities require external services
- No clear degradation path
- Capability groupings may not align with deployment scenarios

**Disqualifying Factor**: Capability-based organization doesn't solve the core problem of managing external dependencies. Users need to know what works offline, what requires local models, and what requires external APIs.

## Decision

We will implement a three-tier feature architecture where features are organized by their external dependency requirements.

### Tier Implementation

**Tier 1: Core Features**

```rust
// Always enabled, no feature flag required
pub const TIER_1_FEATURES: &[&str] = &[
    "memory_capture",           // SQLite persistence
    "memory_search_bm25",       // FTS5 keyword search
    "memory_retrieval",         // Get by ID
    "memory_lifecycle",         // Archive, delete
    "multi_domain_scope",       // Project, user, global
    "mcp_server",               // MCP tool interface
    "cli_interface",            // Command-line operations
];
```

**Tier 2: Enhanced Features**

```rust
// Enabled via configuration, requires local resources
pub struct FeatureFlags {
    pub secrets_filter: bool,        // Pattern-based secret detection
    pub pii_filter: bool,            // Pattern-based PII detection
    pub multi_domain: bool,          // Cross-domain queries
    pub audit_log: bool,             // Audit trail logging
    pub auto_capture: bool,          // Hook-based auto capture
    pub auto_extract_entities: bool, // Entity extraction
    // Tier 2 semantic search requires fastembed-embeddings Cargo feature
}
```

**Tier 3: LLM-Powered Features**

```rust
// Enabled via configuration, requires LLM provider
pub struct FeatureFlags {
    pub llm_features: bool,    // Master toggle for Tier 3
    pub consolidation: bool,   // Memory consolidation
    // Additional Tier 3 features controlled by llm_features
}
```

### Configuration Structure

Configuration follows the tier hierarchy:

```toml
# subcog.toml

[features]
# Tier 2 features (all default to true for core() preset)
secrets_filter = true
pii_filter = true
auto_extract_entities = true

# Tier 3 features (default to false)
llm_features = false
consolidation = false

[llm]
# Only required if llm_features = true
provider = "openai"  # or "anthropic"
model = "gpt-4"
api_key_env = "OPENAI_API_KEY"
```

### Feature Flag Presets

The `FeatureFlags` struct provides convenience presets:

```rust
impl FeatureFlags {
    /// Tier 1: Minimum features, zero dependencies
    pub const fn none() -> Self { /* all flags false */ }

    /// Tier 2: Core + local processing features
    pub const fn core() -> Self {
        Self {
            secrets_filter: true,
            pii_filter: true,
            auto_extract_entities: true,
            // llm_features: false, consolidation: false
        }
    }

    /// Tier 3: All features enabled
    pub const fn all() -> Self { /* all flags true */ }
}
```

## Consequences

### Positive Consequences

1. **Minimal Dependency for Basic Usage**: Users can capture and search memories immediately after installation with zero configuration. The SQLite database is created automatically, and FTS5 indexing is built-in. No model downloads, API keys, or external services are required for Tier 1 functionality.

2. **Explicit Opt-In for Advanced Features**: Features that have cost implications (LLM API calls) or privacy implications (external data transmission) require explicit configuration. Users cannot accidentally incur charges or leak data by misconfiguration.

3. **Graceful Degradation When Services Unavailable**: When the LLM provider is unreachable (network issues, rate limits, provider outage), the system automatically falls back to Tier 2 functionality. When embedding generation fails (model file missing), the system falls back to keyword search. Users always retain the ability to capture and retrieve memories.

4. **Clear Documentation of Requirements**: Each tier has documented requirements:
   - Tier 1: Subcog binary only
   - Tier 2: Subcog binary + `fastembed-embeddings` Cargo feature (includes model)
   - Tier 3: Tier 2 + LLM provider API key

5. **Predictable Resource Requirements**: Each tier has known resource needs:
   - Tier 1: ~10MB disk for SQLite, minimal memory
   - Tier 2: ~500MB disk for embedding model, ~1GB memory for vector index
   - Tier 3: Tier 2 + variable based on LLM usage

6. **Testing Isolation**: Tests can run at specific tier levels without mocking unavailable dependencies. CI/CD can run Tier 1 tests without embedding models or LLM credentials.

### Negative Consequences

1. **Feature Flag Management Complexity**: The codebase must check feature flags at runtime, leading to conditional code paths:

   ```rust
   if config.features.llm_features {
       // Tier 3 code path
   } else if config.features.auto_extract_entities {
       // Tier 2 code path
   } else {
       // Tier 1 code path
   }
   ```

   This complexity is managed through the feature flag abstraction and clear code organization.

2. **Conditional Code Paths**: Multiple code paths for different tiers increase testing burden. Each feature must be tested at each tier level where it applies. Mitigated by clear tier boundaries and preset testing configurations.

3. **Testing All Tier Combinations**: While the tier system reduces combinations from 2^N to 3, there are still edge cases at tier boundaries. Integration tests must verify:
   - Tier 1 standalone operation
   - Tier 1 -> Tier 2 upgrade path
   - Tier 2 -> Tier 1 fallback
   - Tier 2 -> Tier 3 upgrade path
   - Tier 3 -> Tier 2 fallback
   - Tier 3 -> Tier 1 fallback (skip Tier 2)

4. **Feature Placement Debates**: New features require discussion about tier placement. Criteria must be applied consistently:
   - Does it require external network access? -> Tier 3
   - Does it require local models/processing? -> Tier 2
   - Does it work with just SQLite? -> Tier 1

### Neutral Consequences

1. **Cargo Features vs. Runtime Flags**: Some features are controlled at compile time via Cargo features (`fastembed-embeddings`, `usearch-hnsw`), while others are runtime configuration flags. This dual mechanism is necessary because:
   - Cargo features control binary size and compile-time dependencies
   - Runtime flags allow dynamic configuration without recompilation

2. **Default Tier Selection**: The default configuration uses `FeatureFlags::core()` (Tier 2 equivalent), assuming users want semantic search capabilities. Users who want minimal Tier 1 must explicitly configure `FeatureFlags::none()`.

## Decision Outcome

The three-tier feature architecture achieves the goal of allowing users to run Subcog at their desired capability level based on their dependency constraints and privacy requirements.

The tier system provides:
1. **Tier 1**: Zero-dependency operation for maximum portability
2. **Tier 2**: Local-only processing for privacy-conscious semantic search
3. **Tier 3**: Full LLM integration for advanced reasoning capabilities

Users are not forced to configure an LLM provider just to capture and search memories. The tier system makes dependency requirements explicit and enables graceful degradation when services are unavailable.

## Related Decisions

- [ADR-0001: Rust as Implementation Language](adr_0001.md) - Cargo features enable compile-time feature selection
- [ADR-0002: Three-Layer Storage Architecture](adr_0002.md) - Storage backends available at different tiers
- [ADR-0047: Remove Git-Notes Storage Layer](adr_0047.md) - SQLite as Tier 1 persistence

## Implementation Notes

### Feature Flag Initialization

Feature flags are initialized from multiple sources with precedence:

1. Default values (`FeatureFlags::default()`)
2. Configuration file (`subcog.toml` or `~/.config/subcog/config.toml`)
3. Environment variables (`SUBCOG_*`)
4. Command-line arguments (highest precedence)

```rust
impl FeatureFlags {
    pub fn from_config_file(file: &ConfigFileFeatures) -> Self {
        let mut flags = Self::default();
        if let Some(v) = file.secrets_filter { flags.secrets_filter = v; }
        if let Some(v) = file.pii_filter { flags.pii_filter = v; }
        // ... other flags
        flags
    }
}
```

### Runtime Tier Detection

The active tier can be detected at runtime for logging and diagnostics:

```rust
pub fn active_tier(flags: &FeatureFlags) -> u8 {
    if flags.llm_features || flags.consolidation {
        3 // Tier 3: LLM-powered
    } else if flags.auto_extract_entities || flags.secrets_filter {
        2 // Tier 2: Enhanced
    } else {
        1 // Tier 1: Core
    }
}
```

### Fallback Behavior

When a higher-tier feature fails, the system logs the fallback and continues:

```rust
match llm_classify(&content).await {
    Ok(classification) => classification,
    Err(e) => {
        tracing::warn!("LLM classification failed, falling back to keyword: {}", e);
        keyword_classify(&content)  // Tier 2 fallback
    }
}
```

## Links

- [Cargo Features](https://doc.rust-lang.org/cargo/reference/features.html) - Compile-time feature selection
- [Feature Flags Best Practices](https://martinfowler.com/articles/feature-toggles.html) - Martin Fowler on feature toggles

## More Information

- **Date:** 2025-12-28
- **Source:** SPEC-2025-12-28: Subcog Rust Rewrite

## Audit

### 2026-01-04

**Status:** Compliant

**Findings:**

| Finding | Files | Lines | Assessment |
|---------|-------|-------|------------|
| Feature flags define core/enhanced/LLM toggles | `src/config/features.rs` | L3-L67 | compliant |
| SQLite persistence aligns with updated Tier 1 | `src/storage/persistence/sqlite.rs` | - | compliant |
| Feature presets (none, core, all) implemented | `src/config/features.rs` | L33-L80 | compliant |

**Summary:** Feature flags support tiering with SQLite-based persistence per ADR-0047.

**Action Required:** None
