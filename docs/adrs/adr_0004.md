---
title: "Event Bus for Cross-Component Communication"
description: "Central event bus using tokio broadcast channels for loose coupling between components, enabling async state change notifications."
type: adr
category: architecture
tags:
  - event-bus
  - async
  - decoupling
  - broadcast
  - state-management
status: accepted
created: 2025-12-28
updated: 2026-01-04
author: Claude (Architect)
project: subcog
technologies:
  - tokio
  - broadcast-channels
audience:
  - developers
  - architects
confidence: high
completeness: complete
---

# ADR-0004: Event Bus for Cross-Component Communication

## Status

Accepted

## Context

### Background and Problem Statement

Subcog consists of multiple components that need to communicate state changes to each other: the capture service needs to notify the index and vector layers when a new memory is stored, the search service needs to notify metrics collectors when queries complete, the consolidation service needs to update multiple systems when memories are merged, and the hook system needs to trigger downstream processing when content is classified.

The architectural challenge is how to enable this cross-component communication without creating tight coupling between components. Direct method calls between components create dependencies that make the system difficult to test, extend, and maintain.

### The Coupling Problem with Direct Communication

Consider the memory capture flow without an event bus:

```rust
// Tightly coupled capture service
impl CaptureService {
    fn capture(&self, content: &str) -> Result<MemoryId> {
        let memory = self.persistence.store(content)?;

        // Direct coupling to every downstream system
        self.index_service.index(&memory)?;           // Coupling #1
        self.vector_service.embed(&memory)?;          // Coupling #2
        self.metrics_service.record_capture()?;       // Coupling #3
        self.audit_service.log_capture(&memory)?;     // Coupling #4
        self.webhook_service.notify_capture(&memory)?; // Coupling #5

        Ok(memory.id)
    }
}
```

This direct coupling creates several problems:

1. **Dependency Explosion**: The `CaptureService` must know about and depend on every system that cares about captures. Adding a new listener requires modifying the `CaptureService`.

2. **Testing Complexity**: Unit tests for `CaptureService` must mock all five downstream services, even when testing only the persistence logic.

3. **Failure Cascading**: If the webhook service fails, the entire capture operation fails, even though the memory was successfully persisted.

4. **Synchronous Blocking**: Each downstream call blocks the capture operation. A slow webhook endpoint delays the user's capture confirmation.

5. **Circular Dependency Risk**: If the audit service needs to query the capture service, a circular dependency is created.

6. **Ordering Constraints**: The code enforces a specific execution order that may not be necessary or desirable.

### The Event-Driven Alternative

An event bus decouples producers from consumers:

```rust
// Loosely coupled capture service
impl CaptureService {
    fn capture(&self, content: &str) -> Result<MemoryId> {
        let memory = self.persistence.store(content)?;

        // Single point of notification
        self.event_bus.publish(MemoryEvent::Captured {
            memory_id: memory.id,
            namespace: memory.namespace,
            // ...
        });

        Ok(memory.id)
    }
}

// Independent subscriber
impl IndexService {
    async fn handle_events(&self, mut receiver: Receiver<MemoryEvent>) {
        while let Ok(event) = receiver.recv().await {
            if let MemoryEvent::Captured { memory_id, .. } = event {
                self.index(&memory_id).await;
            }
        }
    }
}
```

This decoupling provides:

1. **Single Responsibility**: The capture service only knows about persistence and event publication.

2. **Independent Scaling**: Each subscriber can process events at its own pace.

3. **Failure Isolation**: A failing subscriber doesn't affect the capture operation or other subscribers.

4. **Easy Extension**: New subscribers can be added without modifying the publisher.

5. **Testability**: The capture service can be tested by verifying event publication, without mocking subscribers.

### Event Types in Subcog

Subcog generates events across multiple operation categories:

**Memory Lifecycle Events**:
- `Captured` - A new memory was persisted
- `Retrieved` - A memory was returned from search
- `Updated` - A memory's content or metadata changed
- `Archived` - A memory was soft-deleted
- `Deleted` - A memory was permanently removed
- `Redacted` - Sensitive content was removed from a memory

**Synchronization Events**:
- `Synced` - Memories were synchronized with remote storage

**Processing Events**:
- `Consolidated` - Multiple memories were merged

**MCP Server Events**:
- `McpStarted` - The MCP server began listening
- `McpAuthFailed` - An authentication attempt failed
- `McpToolExecuted` - A tool call completed
- `McpRequestError` - A request processing error occurred

**Hook Events**:
- `HookInvoked` - A hook began execution
- `HookClassified` - Content was classified by a hook
- `HookCaptureDecision` - A hook decided whether to capture
- `HookFailed` - A hook execution failed

### Requirements for the Event System

The event system must satisfy several requirements:

1. **Multiple Subscribers**: Multiple components (index, vector, metrics, audit, webhooks) must receive the same event.

2. **Async Processing**: Subscribers must be able to process events asynchronously without blocking publishers.

3. **Type Safety**: Events must be strongly typed to prevent runtime errors from event schema mismatches.

4. **Observability**: Event publication and consumption must be observable for debugging and monitoring.

5. **Backpressure Handling**: The system must handle slow subscribers gracefully without unbounded memory growth.

6. **Correlation**: Events in a chain (capture -> index -> link) must be correlatable for distributed tracing.

## Decision Drivers

### Primary Decision Drivers

The following factors were weighted most heavily in the event system decision:

1. **Loose Coupling Requirement**: Components must not have direct dependencies on each other. The capture service should not know about or depend on the index service, metrics service, or webhook service. This enables independent development, testing, and deployment of components.

2. **Async-First Architecture**: Subcog uses tokio for async operations throughout. The event system must integrate naturally with async/await patterns and not block the tokio runtime. Subscribers must be able to process events concurrently.

3. **Multiple Subscriber Requirement**: A single event (e.g., `MemoryCaptured`) must be delivered to multiple subscribers (index, vector, metrics, audit, webhooks). The publish-subscribe pattern with broadcast semantics is required.

4. **Failure Isolation Requirement**: A slow or failing subscriber must not affect event publication or other subscribers. The capture operation must complete successfully regardless of downstream processing failures.

5. **Observability Requirement**: Event flow must be observable for debugging and monitoring. Each event must carry metadata (event ID, correlation ID, timestamp, source) that enables tracing event chains across components.

### Secondary Decision Drivers

The following factors influenced the decision but were not individually decisive:

1. **Memory Efficiency**: The event system should not require excessive memory for buffering. A bounded buffer with configurable capacity is preferred over unbounded queues.

2. **Runtime Simplicity**: An in-process event bus is simpler than external message brokers (Redis Pub/Sub, Kafka, RabbitMQ) for the single-binary distribution model.

3. **Testability**: The event bus should be easy to test. Publishers can be tested by subscribing and verifying events. Subscribers can be tested by publishing synthetic events.

4. **Filtering Capability**: Subscribers should be able to filter events by type without receiving and discarding unwanted events.

## Considered Options

### Option 1: Tokio Broadcast Channels (Selected)

**Description**: Use tokio's `broadcast` channel for a multi-producer, multi-consumer event bus with bounded buffering.

**Implementation**:

```rust
use tokio::sync::broadcast;

pub struct EventBus {
    sender: broadcast::Sender<MemoryEvent>,
}

impl EventBus {
    pub fn new(capacity: usize) -> Self {
        let (sender, _receiver) = broadcast::channel(capacity);
        Self { sender }
    }

    pub fn publish(&self, event: MemoryEvent) {
        // Best-effort delivery; logs if no receivers
        let _ = self.sender.send(event);
    }

    pub fn subscribe(&self) -> broadcast::Receiver<MemoryEvent> {
        self.sender.subscribe()
    }
}
```

**Technical Characteristics**:
- Multi-producer: Any component can publish events
- Multi-consumer: Any number of subscribers can receive events
- Bounded buffer: Configurable capacity (default: 1024 events)
- Lagging receivers: Slow receivers skip missed events rather than blocking
- Clone semantics: Events must implement `Clone` for broadcast delivery

**Advantages**:
- Native tokio integration with async/await
- Bounded memory usage with configurable buffer
- Multiple subscribers receive all events
- Lagging semantics prevent slow subscribers from blocking
- No external dependencies (in-process)
- Type-safe event delivery via Rust's type system
- Well-documented and battle-tested

**Disadvantages**:
- Events must implement `Clone` (minor overhead for complex events)
- Lagging receivers miss events (acceptable for non-critical notifications)
- No persistence (events lost on restart)
- Single-process only (no distributed event delivery)

**Risk Assessment**:
- **Technical Risk**: Low. Tokio broadcast is mature and well-understood.
- **Performance Risk**: Low. In-process channels have minimal overhead.
- **Scalability Risk**: Low for single-process; would need replacement for distributed deployment.

### Option 2: Tokio MPSC Channels with Fan-Out

**Description**: Use tokio's `mpsc` (multi-producer, single-consumer) channels with a dedicated fan-out task that forwards events to multiple subscribers.

**Implementation**:

```rust
pub struct EventBus {
    sender: mpsc::Sender<MemoryEvent>,
    subscribers: Arc<Mutex<Vec<mpsc::Sender<MemoryEvent>>>>,
}

impl EventBus {
    pub async fn run_fanout(&self, mut receiver: mpsc::Receiver<MemoryEvent>) {
        while let Some(event) = receiver.recv().await {
            let subscribers = self.subscribers.lock().await;
            for sub in subscribers.iter() {
                let _ = sub.send(event.clone()).await;
            }
        }
    }
}
```

**Advantages**:
- Backpressure propagation to publisher
- Subscriber-specific buffering

**Disadvantages**:
- Requires dedicated fan-out task
- More complex subscription management
- Potential for slow subscriber to affect fan-out task
- Manual subscriber lifecycle management

**Disqualifying Factor**: The added complexity of managing a fan-out task and subscriber registry is not justified when tokio broadcast provides the same semantics out of the box.

### Option 3: External Message Broker (Redis Pub/Sub)

**Description**: Use Redis Pub/Sub for event distribution, enabling distributed event delivery.

**Advantages**:
- Distributed event delivery across processes
- Persistence options (Redis Streams)
- Well-understood operational model
- Language-agnostic subscribers

**Disadvantages**:
- External dependency violates single-binary requirement
- Network latency for every event
- Operational complexity (Redis deployment, monitoring)
- Serialization overhead for every event
- Overkill for single-process deployment

**Disqualifying Factor**: The single-binary distribution model requires zero external dependencies. Redis would add operational complexity that is not justified for the current deployment model.

### Option 4: Callback Registry Pattern

**Description**: Maintain a registry of callback functions that are invoked synchronously when events occur.

**Implementation**:

```rust
pub struct EventBus {
    callbacks: Vec<Box<dyn Fn(&MemoryEvent) + Send + Sync>>,
}

impl EventBus {
    pub fn publish(&self, event: MemoryEvent) {
        for callback in &self.callbacks {
            callback(&event);
        }
    }
}
```

**Advantages**:
- Simple implementation
- No buffering overhead
- Synchronous execution for ordered processing

**Disadvantages**:
- Synchronous execution blocks publisher
- Slow callback affects all subsequent callbacks
- Callback failure can crash publisher
- No async support without spawning tasks

**Disqualifying Factor**: Synchronous callback execution would block the capture operation on every downstream processing step, violating the async-first architecture requirement.

### Option 5: Actor Model (Actix)

**Description**: Use the actor model with message passing for component communication.

**Advantages**:
- Strong isolation between components
- Supervised failure recovery
- Location transparency for distribution

**Disadvantages**:
- Significant architectural overhead
- Learning curve for actor patterns
- Overkill for simple event notification
- Additional dependency (actix crate)

**Disqualifying Factor**: The actor model provides capabilities (supervision, location transparency) that are not required for simple event notification. The complexity is not justified for the current use case.

## Decision

We will implement a central event bus using tokio broadcast channels for cross-component communication.

### Event Bus Implementation

The event bus is implemented as a simple wrapper around tokio's broadcast channel:

```rust
use tokio::sync::broadcast;
use std::sync::OnceLock;

const DEFAULT_EVENT_BUS_CAPACITY: usize = 1024;

#[derive(Clone)]
pub struct EventBus {
    sender: broadcast::Sender<MemoryEvent>,
}

impl EventBus {
    /// Creates a new event bus with the given buffer capacity.
    pub fn new(capacity: usize) -> Self {
        let (sender, _receiver) = broadcast::channel(capacity);
        Self { sender }
    }

    /// Publishes an event to all subscribers (best effort).
    pub fn publish(&self, event: MemoryEvent) {
        metrics::counter!("event_bus_publish_total").increment(1);
        match self.sender.send(event) {
            Ok(_) => {
                metrics::gauge!("event_bus_queue_depth")
                    .set(self.sender.len() as f64);
            }
            Err(_) => {
                metrics::counter!("event_bus_publish_failed_total").increment(1);
            }
        }
    }

    /// Subscribes to the event bus.
    pub fn subscribe(&self) -> broadcast::Receiver<MemoryEvent> {
        metrics::counter!("event_bus_subscriptions_total").increment(1);
        self.sender.subscribe()
    }

    /// Subscribes with a filter predicate.
    pub fn subscribe_filtered<F>(&self, predicate: F) -> FilteredReceiver<F>
    where
        F: Fn(&MemoryEvent) -> bool,
    {
        FilteredReceiver {
            receiver: self.sender.subscribe(),
            predicate,
        }
    }

    /// Subscribes to events of a specific type.
    pub fn subscribe_event_type(
        &self,
        event_type: &'static str,
    ) -> FilteredReceiver<impl Fn(&MemoryEvent) -> bool> {
        self.subscribe_filtered(move |event| event.event_type() == event_type)
    }
}

// Global event bus singleton
static GLOBAL_EVENT_BUS: OnceLock<EventBus> = OnceLock::new();

pub fn global_event_bus() -> &'static EventBus {
    GLOBAL_EVENT_BUS.get_or_init(|| EventBus::new(DEFAULT_EVENT_BUS_CAPACITY))
}
```

### Event Structure

Events carry metadata for observability and correlation:

```rust
/// Shared event metadata required for observability.
#[derive(Debug, Clone)]
pub struct EventMeta {
    /// Unique identifier for this event.
    pub event_id: String,
    /// Optional correlation identifier for request/trace linking.
    pub correlation_id: Option<String>,
    /// Event source component.
    pub source: &'static str,
    /// Timestamp (Unix epoch seconds).
    pub timestamp: u64,
}

/// Events emitted during memory operations.
#[derive(Debug, Clone)]
pub enum MemoryEvent {
    Captured {
        meta: EventMeta,
        memory_id: MemoryId,
        namespace: Namespace,
        domain: Domain,
        content_length: usize,
    },
    Retrieved {
        meta: EventMeta,
        memory_id: MemoryId,
        query: Arc<str>,  // Arc for zero-copy sharing
        score: f32,
    },
    // ... other event variants
}

impl MemoryEvent {
    /// Returns the event type name for filtering.
    pub const fn event_type(&self) -> &'static str {
        match self {
            Self::Captured { .. } => "captured",
            Self::Retrieved { .. } => "retrieved",
            // ... other variants
        }
    }

    /// Returns the event metadata.
    pub const fn meta(&self) -> &EventMeta {
        match self {
            Self::Captured { meta, .. } => meta,
            Self::Retrieved { meta, .. } => meta,
            // ... other variants
        }
    }
}
```

### Subscriber Pattern

Subscribers run as async tasks that process events from the bus:

```rust
impl AuditService {
    pub async fn run(&self, event_bus: &EventBus) {
        let mut receiver = event_bus.subscribe();

        loop {
            match receiver.recv().await {
                Ok(event) => {
                    self.log_event(&event).await;
                }
                Err(broadcast::error::RecvError::Lagged(count)) => {
                    tracing::warn!("Audit service lagged, missed {} events", count);
                    metrics::counter!("event_bus_lagged_total").increment(count);
                }
                Err(broadcast::error::RecvError::Closed) => {
                    tracing::info!("Event bus closed, audit service shutting down");
                    break;
                }
            }
        }
    }
}
```

### Filtered Receiver

For subscribers that only care about specific event types:

```rust
pub struct FilteredReceiver<F> {
    receiver: broadcast::Receiver<MemoryEvent>,
    predicate: F,
}

impl<F> FilteredReceiver<F>
where
    F: Fn(&MemoryEvent) -> bool,
{
    /// Receives the next event that matches the predicate.
    pub async fn recv(&mut self) -> Result<MemoryEvent, broadcast::error::RecvError> {
        loop {
            match self.receiver.recv().await {
                Ok(event) if (self.predicate)(&event) => return Ok(event),
                Ok(_) => continue,  // Skip non-matching events
                Err(broadcast::error::RecvError::Lagged(skipped)) => {
                    metrics::counter!("event_bus_lagged_total").increment(skipped);
                    // Continue trying to receive
                }
                Err(err) => return Err(err),
            }
        }
    }
}
```

## Consequences

### Positive Consequences

1. **Loose Coupling Between Components**: Components communicate through events rather than direct method calls. The capture service publishes `MemoryCaptured` events without knowing which components consume them. New consumers (e.g., a new analytics service) can be added without modifying publishers.

2. **Easy Addition of New Event Handlers**: Adding a new handler requires only subscribing to the event bus and processing relevant events. No changes to existing code are required:

   ```rust
   // New service added without touching CaptureService
   impl NewAnalyticsService {
       pub async fn run(&self, event_bus: &EventBus) {
           let mut receiver = event_bus.subscribe_event_type("captured");
           while let Ok(event) = receiver.recv().await {
               self.process_capture(&event).await;
           }
       }
   }
   ```

3. **Centralized Event Logging and Observability**: All events flow through a single bus that instruments publication and consumption. Metrics track:
   - `event_bus_publish_total` - Total events published
   - `event_bus_publish_failed_total` - Failed publications (no receivers)
   - `event_bus_subscriptions_total` - Total subscriptions created
   - `event_bus_queue_depth` - Current buffer utilization
   - `event_bus_lagged_total` - Events missed by slow receivers
   - `event_bus_receivers` - Current active receiver count

4. **Testable in Isolation**: Publishers are tested by subscribing to the bus and verifying published events. Subscribers are tested by publishing synthetic events:

   ```rust
   #[tokio::test]
   async fn test_capture_publishes_event() {
       let bus = EventBus::new(16);
       let mut receiver = bus.subscribe_event_type("captured");

       let service = CaptureService::new(bus.clone());
       service.capture("test content").await.unwrap();

       let event = receiver.recv().await.unwrap();
       assert_eq!(event.event_type(), "captured");
   }
   ```

5. **Async Non-Blocking Publication**: Event publication is non-blocking. The `publish` method returns immediately after enqueueing the event, allowing the capture operation to complete without waiting for downstream processing.

6. **Correlation ID Propagation**: The `EventMeta.correlation_id` field enables tracing event chains. When a capture triggers indexing which triggers linking, all events share the same correlation ID for distributed tracing reconstruction.

### Negative Consequences

1. **Eventual Consistency Between Components**: Events are processed asynchronously, meaning state changes are not immediately visible across components. This creates specific user-facing scenarios:

   - **Search After Capture**: A user captures a memory and immediately searches for it, but the memory is not yet indexed. The search returns no results even though the capture succeeded.

   - **Metrics Lag**: Dashboard metrics may show stale counts until the metrics handler processes queued events.

   - **Linking Delay**: Related memories may not be linked until the linking handler processes the capture event.

   **Mitigation Strategies**:
   - Return the captured memory directly from the capture operation ("read your writes")
   - Document expected propagation delays in API documentation
   - For critical paths requiring immediate consistency, bypass the event bus and call services directly
   - Implement "sync" operations that wait for event processing to complete

2. **Debugging Event Chains Is Challenging**: When an event triggers handlers that emit additional events, tracing the full execution path becomes difficult. A capture event might trigger:
   - `MemoryCaptured` -> `IndexUpdated` -> `VectorEmbeddingGenerated` -> `SimilarMemoriesLinked`

   Failures in later stages are difficult to correlate with the original trigger without proper tooling.

   **Mitigation Strategies**:
   - Propagate `correlation_id` through all events in a chain
   - Use structured logging with correlation ID in every handler
   - Integrate with OpenTelemetry for distributed tracing visualization
   - Implement event chain visualization in debugging tools

3. **Potential Event Storms from Bulk Operations**: Bulk operations (importing 1000 memories, full reindex, consolidation runs) can flood the event bus, potentially:
   - Exhausting the buffer (causing lagged receivers)
   - Overwhelming downstream handlers
   - Creating CPU/memory spikes

   **Mitigation Strategies**:
   - Configure appropriate buffer size (default: 1024 events)
   - Implement batching for bulk operations (emit one `BulkCaptured` event instead of 1000 `Captured` events)
   - Throttle event emission rate during bulk operations
   - Monitor metrics and alert on:
     - `event_bus_queue_depth > 800` (80% utilization)
     - `event_bus_lagged_total > 0` (events being dropped)
   - Consider dedicated queues for bulk operations

4. **Lost Events on Process Restart**: The in-process event bus does not persist events. If the process crashes after publishing an event but before all subscribers process it, those events are lost.

   **Mitigation Strategies**:
   - Critical operations (persistence writes) complete before event publication
   - Implement recovery procedures that rebuild derived state (reindex, re-embed)
   - For truly critical events, consider write-ahead logging before publication

5. **Buffer Sizing Trade-offs**: The broadcast channel has a fixed buffer size. Too small causes lagging; too large wastes memory.
   - Default: 1024 events
   - Each event: ~200-500 bytes depending on content
   - Buffer memory: ~200KB-500KB at capacity

### Neutral Consequences

1. **Events Must Implement Clone**: Tokio broadcast requires `Clone` for events because each subscriber receives its own copy. This is implemented via `#[derive(Clone)]` on `MemoryEvent` and all nested types. For large data (memory content), use `Arc<str>` to share the underlying data.

2. **Global Event Bus Singleton**: The `global_event_bus()` function provides a process-wide singleton. This simplifies integration but requires care in testing to avoid cross-test interference. Tests should create isolated event bus instances.

3. **Fire-and-Forget Semantics**: Event publication is best-effort. If there are no subscribers, events are silently dropped (logged to metrics). This is intentional - publishers should not fail because no one is listening.

## Decision Outcome

The tokio broadcast event bus provides the right semantics for cross-component communication in Subcog:

1. **Multiple Receivers**: All interested components receive every event
2. **Async Processing**: Subscribers process events without blocking publishers
3. **Bounded Memory**: Configurable buffer prevents unbounded growth
4. **Lagging Semantics**: Slow subscribers skip events rather than blocking
5. **Native Integration**: Works naturally with tokio's async runtime

The event-driven architecture enables loose coupling between components while maintaining observability through centralized event flow and correlation IDs.

## Related Decisions

- [ADR-0001: Rust as Implementation Language](adr_0001.md) - Tokio provides the broadcast channel implementation
- [ADR-0002: Three-Layer Storage Architecture](adr_0002.md) - Storage layers communicate via events
- [ADR-0003: Feature Tier System](adr_0003.md) - Event handlers can be feature-gated

## Implementation Notes

### Event Bus Initialization

The event bus is initialized at application startup:

```rust
fn main() {
    // Initialize global event bus
    let _bus = global_event_bus();

    // Start event subscribers
    tokio::spawn(audit_service.run(global_event_bus()));
    tokio::spawn(metrics_service.run(global_event_bus()));
    tokio::spawn(webhook_service.run(global_event_bus()));

    // Start MCP server (publishes events)
    mcp_server.run().await;
}
```

### Testing with Event Bus

Tests create isolated event bus instances:

```rust
#[tokio::test]
async fn test_subscriber_receives_events() {
    let bus = EventBus::new(16);  // Isolated instance
    let mut receiver = bus.subscribe();

    bus.publish(MemoryEvent::Captured { /* ... */ });

    let event = tokio::time::timeout(
        Duration::from_millis(100),
        receiver.recv()
    ).await.unwrap().unwrap();

    assert_eq!(event.event_type(), "captured");
}
```

### Graceful Shutdown

On shutdown, the event bus sender is dropped, which closes all receivers:

```rust
impl EventBus {
    pub fn shutdown(&self) {
        // Dropping sender closes all receivers
        // Receivers will receive RecvError::Closed
    }
}
```

## Links

- [tokio::sync::broadcast](https://docs.rs/tokio/latest/tokio/sync/broadcast/index.html) - Tokio broadcast channel documentation
- [Event-Driven Architecture](https://martinfowler.com/articles/201701-event-driven.html) - Martin Fowler on event-driven patterns

## More Information

- **Date:** 2025-12-28
- **Source:** SPEC-2025-12-28: Subcog Rust Rewrite

## Audit

### 2026-01-04

**Status:** Violated

**Findings:**

| Finding | Files | Lines | Assessment |
|---------|-------|-------|------------|
| Events are recorded directly via audit logger | `src/security/audit.rs` | L697-L701 | violation |

**Summary:** No central event bus exists; events are logged directly without broadcast subscribers.

**Action Required:** Implement a tokio broadcast event bus or revise the ADR to match current event handling.

### 2026-01-04

**Status:** Compliant

**Findings:**

| Finding | Files | Lines | Assessment |
|---------|-------|-------|------------|
| Tokio broadcast event bus added for memory events | `src/observability/event_bus.rs` | L1-L41 | compliant |
| Audit logger subscribes to event bus and publishes events | `src/security/audit.rs` | L700-L717 | compliant |
| EventMeta with correlation_id for tracing | `src/models/events.rs` | L9-L43 | compliant |
| MemoryEvent enum with all event types | `src/models/events.rs` | L45-L212 | compliant |
| FilteredReceiver for type-based subscription | `src/observability/event_bus.rs` | L57-L98 | compliant |
| Global event bus singleton | `src/observability/event_bus.rs` | L100-L106 | compliant |
| Metrics for event bus observability | `src/observability/event_bus.rs` | L36-L46 | compliant |

**Summary:** Memory events are broadcast through a central event bus with audit logging handled via subscription. Full event metadata and filtering support implemented.

**Action Required:** None
