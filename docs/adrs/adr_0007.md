---
title: "fastembed for Embedding Generation"
description: "Use fastembed crate with all-MiniLM-L6-v2 model for local, offline-capable text embedding generation."
type: adr
category: ai-ml
tags:
  - embeddings
  - fastembed
  - semantic-search
  - local-inference
  - ml
status: published
created: 2025-12-28
updated: 2026-01-04
author: Claude (Architect)
project: subcog
related:
  - docs/adrs/adr_0037.md
technologies:
  - fastembed
  - minilm
  - onnx
audience:
  - developers
  - architects
confidence: high
completeness: complete
---

# ADR-0007: fastembed for Embedding Generation

## Status

Accepted

## Context

The system needs to generate text embeddings for semantic search.

## Options Considered

### Option 1: Call external API (OpenAI, Cohere)

**Pros:**
- High quality embeddings
- No local compute needed

**Cons:**
- Network dependency
- API costs
- Latency

### Option 2: Use Python bridge (sentence-transformers)

**Pros:**
- High quality models
- Wide ecosystem

**Cons:**
- Python dependency
- Complex integration

### Option 3: Use native Rust library (fastembed)

**Pros:**
- No external dependency
- Works offline
- Fast inference

**Cons:**
- Model download on first use
- Larger binary size

## Decision

We will use the fastembed crate with all-MiniLM-L6-v2 model (384 dimensions).

## Consequences

### Positive

- No external API dependency for core features
- Works offline
- Single binary (model embedded or cached)
- Fast inference

### Negative

- Model download on first use
- Fixed model (no custom training)
- Larger binary size

## Decision Outcome

Local embedding generation aligns with the local-first philosophy. fastembed provides the best native Rust experience.

## More Information

- **Date:** 2025-12-28
- **Source:** SPEC-2025-12-28: Subcog Rust Rewrite
- **Related ADRs:** [ADR-0037](adr_0037.md) (Model Selection)
