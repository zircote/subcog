---
title: "usearch for Vector Search"
description: "Use usearch HNSW library for vector similarity search with excellent performance and small memory footprint."
type: adr
category: search
tags:
  - vector-search
  - usearch
  - hnsw
  - similarity-search
  - knn
status: accepted
created: 2025-12-28
updated: 2026-01-04
author: Claude (Architect)
project: subcog
related:
  - docs/adrs/adr_0038.md
technologies:
  - usearch
  - hnsw
  - sqlite
audience:
  - developers
  - architects
confidence: high
completeness: complete
---

# ADR-0008: usearch for Vector Search

## Status

Accepted

## Context

### Problem Statement

Subcog's semantic search functionality requires finding memories that are similar to a query based on vector embeddings (see ADR-0007). Given a query embedding (384-dimensional float vector), the system must efficiently retrieve the top-K most similar memory embeddings from a potentially large collection. This is the k-Nearest Neighbors (k-NN) problem in high-dimensional space.

### Why This Decision Was Needed

Vector similarity search is computationally expensive. A naive brute-force approach computes the distance between the query vector and every stored vector, resulting in O(n) time complexity where n is the number of memories. For a collection of 10,000 memories with 384-dimensional vectors:

- **Brute force**: 10,000 distance computations x 384 floating-point operations = 3.84 million operations per query
- **At 1 GFLOP**: ~4ms per query (acceptable)
- **At 100,000 memories**: ~40ms per query (noticeable delay)
- **At 1,000,000 memories**: ~400ms per query (unacceptable)

As memory collections grow, brute-force becomes impractical. An index structure that provides sub-linear search time is essential for scalability.

### Technical Requirements

The vector search solution must satisfy:

1. **Sub-linear Search Time**: Query time must scale better than O(n) as collection size grows.
2. **High Recall**: Must return the actual nearest neighbors, not just approximate ones, with >95% accuracy.
3. **Low Memory Footprint**: Index size should be proportional to data size, not exponentially larger.
4. **Incremental Updates**: Must support adding and removing vectors without full index rebuilds.
5. **Persistence**: Index must be saveable to disk and reloadable efficiently.
6. **Rust Integration**: Must have Rust bindings or be implementable in pure Rust.
7. **No External Server**: Must run in-process, not require a separate database server.

### Similarity Metrics

For normalized embedding vectors (which fastembed produces), the relevant similarity metrics are:

| Metric | Formula | Range | Notes |
|--------|---------|-------|-------|
| Cosine Similarity | dot(a,b) / (||a|| * ||b||) | [-1, 1] | For normalized vectors, equals dot product |
| Dot Product | sum(a[i] * b[i]) | (-inf, inf) | Fastest, equivalent to cosine for normalized |
| Euclidean Distance | sqrt(sum((a[i]-b[i])^2)) | [0, inf] | L2 distance, lower is more similar |
| Angular Distance | arccos(cosine) / pi | [0, 1] | Normalized angle between vectors |

For Subcog, **cosine similarity** is used because:
- Embeddings are L2-normalized by fastembed
- Cosine similarity is intuitive (1 = identical, 0 = orthogonal, -1 = opposite)
- Most embedding benchmarks (MTEB) report cosine similarity performance

### Index Structure Options

Several algorithmic approaches exist for approximate nearest neighbor (ANN) search:

| Algorithm | Search Time | Index Size | Build Time | Recall@10 | Update Support |
|-----------|-------------|------------|------------|-----------|----------------|
| Brute Force | O(n) | O(n) | O(n) | 100% | Trivial |
| LSH (Locality-Sensitive Hashing) | O(1) avg | O(n) | O(n) | 70-90% | Moderate |
| IVF (Inverted File Index) | O(sqrt(n)) | O(n) | O(n log n) | 90-95% | Requires retrain |
| **HNSW** (Hierarchical NSW) | O(log n) | O(n log n) | O(n log n) | 95-99% | Good |
| Annoy (Approximate NN Oh Yeah) | O(log n) | O(n) | O(n log n) | 90-95% | Append-only |
| ScaNN (Google) | O(sqrt(n)) | O(n) | O(n) | 95-99% | Limited |

## Decision Drivers

### Primary Drivers

1. **HNSW Algorithm Superiority**: For Subcog's workload (10K-100K vectors, 384 dimensions, frequent updates), HNSW provides the best balance of search speed, recall, and update flexibility. The O(log n) search time ensures consistent performance as collections grow.

2. **Local-First Architecture**: Subcog runs entirely on the user's machine without external services. The vector index must be an in-process library, not a client to a vector database server. This eliminates options like Pinecone, Weaviate, Milvus, and Qdrant (server mode).

3. **Single-File Persistence**: The index should be storable in a single file alongside the SQLite database, simplifying backup, migration, and version control. Solutions requiring multiple files or directory structures add operational complexity.

4. **Incremental Updates**: Memories are added and deleted frequently. The index must support upsert and delete operations without requiring full rebuilds. HNSW handles this well; IVF and Annoy do not.

5. **Memory Efficiency**: The index must fit in RAM on typical developer machines (8-16GB). HNSW's O(n log n) space is acceptable; approaches requiring O(n^2) space are not.

### Secondary Drivers

6. **Rust Ecosystem Maturity**: The solution should have production-quality Rust bindings with active maintenance. Experimental or unmaintained libraries increase long-term risk.

7. **Cross-Platform Support**: Must work on macOS (Intel + Apple Silicon), Linux (x86_64 + aarch64), and Windows without platform-specific code paths.

8. **Query-Time Tuning**: The ability to adjust recall/speed tradeoff at query time (via `ef` parameter in HNSW) enables optimization for different use cases.

## HNSW Algorithm Selection

### Why HNSW?

Hierarchical Navigable Small World (HNSW) graphs provide the best general-purpose ANN performance:

| Property | HNSW Advantage |
|----------|----------------|
| Search complexity | O(log n) - scales excellently |
| Recall | 95-99% with proper tuning |
| Update support | Add/delete without rebuild |
| Memory usage | ~1.5KB per vector (384 dims) |
| Build time | Incremental, not batch-only |
| Tuning | Both build-time and query-time parameters |

### HNSW Algorithm Overview

HNSW constructs a multi-layer graph where:

1. **Layer 0**: Contains all vectors, densely connected
2. **Higher layers**: Contain progressively fewer vectors (exponentially decreasing), with long-range connections
3. **Search process**: Start at top layer, greedily navigate toward query, descend layers, refine at layer 0

```
Layer 2:  A -------- B -------- C         (few nodes, long edges)
          |          |          |
Layer 1:  A -- D --- B --- E -- C         (more nodes, medium edges)
          |    |     |    |     |
Layer 0:  A-F-D-G-H-B-I-J-E-K-L-C         (all nodes, short edges)
```

**Key Parameters**:

| Parameter | Role | Impact |
|-----------|------|--------|
| M | Edges per node | Higher = better recall, more memory |
| efConstruction | Build-time search width | Higher = better index quality, slower build |
| ef | Query-time search width | Higher = better recall, slower query |

### Algorithm Comparison Details

#### Brute Force

```
for each vector v in collection:
    distance = compute_distance(query, v)
    if distance in top_k:
        update_top_k(v, distance)
```

- **Time**: O(n * d) where d = dimensions
- **Space**: O(n * d) for storage only
- **Recall**: 100% (exact)
- **When to use**: <1,000 vectors, or when exact results required

#### LSH (Locality-Sensitive Hashing)

```
for each hash_function h in hash_family:
    bucket = h(query)
    candidates += vectors_in_bucket[bucket]
for each candidate in candidates:
    distance = compute_distance(query, candidate)
    update_top_k(candidate, distance)
```

- **Time**: O(1) average for hash lookup, O(candidates * d) for verification
- **Space**: O(n * num_hash_tables)
- **Recall**: 70-90% depending on parameters
- **When to use**: Very high dimensions, approximate OK

#### IVF (Inverted File Index)

```
// Build: cluster vectors into k centroids
// Search:
nearest_centroids = find_nearest_centroids(query, nprobe)
for each centroid in nearest_centroids:
    for each vector in centroid.vectors:
        distance = compute_distance(query, vector)
        update_top_k(vector, distance)
```

- **Time**: O(nprobe * vectors_per_centroid)
- **Space**: O(n * d + k * d) for vectors + centroids
- **Recall**: 90-95% with sufficient nprobe
- **When to use**: Large static datasets, batch updates acceptable

#### HNSW (Selected)

```
// Search:
entry_point = top_layer_entry
for layer in [max_layer..0]:
    entry_point = greedy_search(query, entry_point, layer, ef=1)
candidates = greedy_search(query, entry_point, layer=0, ef=ef_search)
return top_k(candidates)
```

- **Time**: O(log n * ef) for search
- **Space**: O(n * M * layers) for graph edges
- **Recall**: 95-99% with proper ef
- **When to use**: General purpose, dynamic datasets

## usearch vs hnswlib Comparison

Two mature HNSW implementations exist with Rust bindings:

| Feature | usearch | hnswlib |
|---------|---------|---------|
| **Rust bindings** | Native, first-class | FFI wrapper, community-maintained |
| **Memory-mapped I/O** | Yes, built-in | No, requires custom implementation |
| **Quantization** | int8, f16, f32 | f32 only |
| **Development status** | Active (2024+) | Maintenance mode |
| **Index file format** | Single file, portable | Single file |
| **Multi-threaded build** | Yes | Yes |
| **Filtered search** | Yes | Limited |
| **API ergonomics** | Modern Rust API | C-style FFI |
| **Binary size** | ~2MB | ~1.5MB |
| **Last release** | 2024 (ongoing) | 2023 |

### Why usearch Over hnswlib

1. **First-Class Rust Support**: usearch was designed with Rust as a primary language, not an afterthought. The API is idiomatic Rust with proper error handling via `Result`.

2. **Memory-Mapped Loading**: usearch supports `view()` to memory-map an index file, enabling large indexes to be used with limited RAM. hnswlib requires loading the entire index into memory.

3. **Active Development**: usearch receives regular updates with new features and optimizations. hnswlib is in maintenance mode with infrequent releases.

4. **Quantization Support**: usearch can store vectors as int8 or f16, reducing memory usage by 2-4x. This is valuable for large collections on memory-constrained machines.

5. **Better Documentation**: usearch has comprehensive documentation and examples specifically for Rust usage.

## Considered Options

### Option 1: sqlite-vec (SQLite Extension) (Rejected)

**Description**: Use the `sqlite-vec` extension to store vectors directly in SQLite with built-in similarity search.

**Technical Details**:
- Extension: `sqlite-vec` (https://github.com/asg017/sqlite-vec)
- Storage: Vectors stored as BLOBs in SQLite tables
- Search: SQL functions like `vec_distance_cosine()`

**Pros**:
- Single file storage (vectors with metadata)
- ACID transactions (vector updates are atomic)
- SQL interface (familiar query language)
- No additional dependencies beyond SQLite

**Cons**:
- Relatively new project (less battle-tested)
- Performance concerns for large collections (brute-force or simple indexing)
- Limited tuning options
- Bundled SQLite version may conflict with rusqlite

**Performance Characteristics**:
- 10K vectors: ~15ms query (acceptable)
- 100K vectors: ~150ms query (slow)
- No ANN index, relies on SQLite's query optimizer

**Why Rejected**: Performance does not scale well for larger collections. The lack of a true ANN index means query time grows linearly with collection size.

### Option 2: usearch (Standalone HNSW) (Accepted)

**Description**: Use the `usearch` crate for standalone HNSW-based vector similarity search.

**Technical Details**:
- Crate: `usearch` (https://crates.io/crates/usearch)
- Algorithm: HNSW (Hierarchical Navigable Small World)
- Storage: Single `.usearch` file + `.meta.json` for ID mappings

**Implementation**:
```rust
use usearch::{Index, IndexOptions, MetricKind, ScalarKind};

let options = IndexOptions {
    dimensions: 384,
    metric: MetricKind::Cos,
    quantization: ScalarKind::F32,
    connectivity: 16,        // M parameter
    expansion_add: 128,      // efConstruction
    expansion_search: 64,    // ef (default)
    multi: false,
};

let index = Index::new(&options)?;
index.add(key, &embedding)?;
let results = index.search(&query, limit)?;
```

**Pros**:
- Excellent performance (<5ms for 10K vectors)
- Well-maintained with regular updates
- Small memory footprint
- Memory-mapped loading for large indexes
- Native Rust bindings

**Cons**:
- Separate file from SQLite (two files to manage)
- No ACID guarantees (crash during write can corrupt)
- Requires manual sync between SQLite and usearch

**Why Accepted**: Best performance/simplicity tradeoff for local usage. The lack of ACID is acceptable given git notes as the authoritative data source (can rebuild index from notes).

### Option 3: lance (Columnar + Vector) (Rejected)

**Description**: Use Lance, a columnar data format with built-in vector search capabilities.

**Technical Details**:
- Library: `lance` (https://github.com/lancedb/lance)
- Format: Apache Arrow-based columnar storage
- Search: IVF-PQ (Inverted File with Product Quantization)

**Pros**:
- Combined storage for metadata and vectors
- Columnar format efficient for analytics
- Version control built-in
- Active development by LanceDB team

**Cons**:
- New paradigm (not just a vector index)
- Larger dependency footprint
- Different storage model than SQLite
- Would require significant architecture changes

**Why Rejected**: Adopting Lance would require rethinking the entire storage layer. The benefits don't justify abandoning the proven SQLite + separate index approach.

### Option 4: qdrant (Vector Database Server) (Rejected)

**Description**: Use Qdrant as a vector database server accessed via client library.

**Technical Details**:
- Server: Qdrant (https://qdrant.tech/)
- Client: `qdrant-client` crate
- Protocol: gRPC or REST API

**Pros**:
- Feature-rich (filtering, payload storage, clustering)
- Production-ready with high availability
- Excellent documentation
- Active community

**Cons**:
- Requires external server process
- Operational complexity (startup, shutdown, monitoring)
- Network overhead for local usage
- Violates single-binary distribution goal

**Why Rejected**: Fundamentally incompatible with Subcog's single-binary, no-external-dependencies architecture. Users should not need to run a database server.

### Option 5: hnswlib (Original HNSW) (Rejected)

**Description**: Use hnswlib, the original HNSW implementation, via Rust FFI bindings.

**Technical Details**:
- Library: hnswlib (https://github.com/nmslib/hnswlib)
- Bindings: Community-maintained Rust wrapper
- Interface: C-style FFI

**Pros**:
- Original HNSW implementation, well-proven
- Slightly smaller binary size
- Wide adoption

**Cons**:
- Rust bindings are community-maintained, not official
- C-style API (manual memory management)
- No memory-mapped loading
- Development has slowed significantly

**Why Rejected**: usearch provides a better Rust experience with equivalent or better performance. The maintenance status of hnswlib Rust bindings is concerning.

### Option 6: Pure Rust Fallback (Implemented as Backup)

**Description**: Implement a brute-force vector search in pure Rust as a fallback when usearch is not available.

**Technical Details**:
- No external dependencies
- O(n) brute-force search
- HashMap storage for vectors

**Implementation**:
```rust
fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    let dot: f32 = a.iter().zip(b).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    dot / (norm_a * norm_b)
}
```

**Pros**:
- Zero external dependencies
- Always available
- Simple implementation

**Cons**:
- O(n) performance (slow for large collections)
- No incremental indexing benefits
- Memory-heavy (all vectors in HashMap)

**Status**: Implemented as fallback when `usearch-hnsw` feature is disabled. Suitable for small collections (<1,000 vectors) or when native dependencies are problematic.

## Decision

We will use usearch for the default SQLite+usearch backend with the following configuration:

### Index Configuration

```rust
IndexOptions {
    dimensions: 384,           // MiniLM-L6-v2 output size
    metric: MetricKind::Cos,   // Cosine similarity for normalized embeddings
    quantization: ScalarKind::F32,  // Full precision, ~1.5KB per vector
    connectivity: 16,          // M parameter: edges per node
    expansion_add: 128,        // efConstruction: build-time search width
    expansion_search: 64,      // ef: query-time search width
    multi: false,              // Single vector per key
}
```

### Parameter Rationale

#### M = 16 (Connectivity)

The M parameter controls how many edges each node has in the HNSW graph:

| M Value | Memory per Vector | Recall@10 | Search Speed |
|---------|-------------------|-----------|--------------|
| 8 | 0.75 KB | 92% | Fastest |
| **16** | **1.5 KB** | **97%** | **Fast** |
| 32 | 3 KB | 99% | Moderate |
| 64 | 6 KB | 99.5% | Slow |

**M=16** is the standard choice for 384-dimensional vectors, balancing memory usage against recall. Higher values (32, 64) provide diminishing returns in recall while significantly increasing memory.

For Subcog's typical collection sizes (10K-100K vectors):
- M=16: Index size ~15-150 MB
- M=32: Index size ~30-300 MB

#### efConstruction = 128 (Build-Time Quality)

The efConstruction parameter controls index quality during construction:

| efConstruction | Build Time (10K vectors) | Recall@10 |
|----------------|--------------------------|-----------|
| 64 | ~1.2s | 94% |
| **128** | **~2.0s** | **98%** |
| 256 | ~3.5s | 99% |
| 512 | ~6.0s | 99.5% |

**efConstruction=128** provides excellent recall (98%) without excessive build time. Since memories are added incrementally (not in bulk), build time per-vector is the relevant metric.

#### ef = 64 (Query-Time Recall)

The ef parameter controls recall/speed tradeoff at query time:

| ef | Query Time (10K vectors) | Recall@10 |
|----|--------------------------|-----------|
| 32 | ~2ms | 93% |
| **64** | **~4ms** | **97%** |
| 128 | ~8ms | 99% |
| 256 | ~15ms | 99.5% |

**ef=64** achieves 97% recall with <5ms query time. This can be adjusted per-query if higher precision is needed for specific operations.

### File Layout

```
.subcog/
  memory.db           # SQLite database (memories, metadata, FTS5 index)
  vectors.usearch     # usearch HNSW index (binary)
  vectors.meta.json   # ID mappings (memory_id <-> usearch_key)
```

### Feature Flag

usearch support is gated behind the `usearch-hnsw` feature flag:

```toml
# Cargo.toml
[features]
usearch-hnsw = ["dep:usearch"]
```

When disabled, the pure-Rust brute-force fallback is used.

## Consequences

### Positive

1. **Excellent Performance**: Query time is <10ms for 10K vectors, <50ms for 100K vectors. This ensures snappy search responses even for large memory collections.

2. **Small Memory Footprint**: With M=16 and F32 quantization, each vector consumes ~1.5KB in the index. A 10K vector collection uses ~15MB of RAM.

3. **No External Server**: usearch runs entirely in-process. No database server to start, configure, or monitor.

4. **Well-Maintained Library**: usearch is actively developed with regular releases, reducing the risk of dependency rot.

5. **Memory-Mapped Loading**: Large indexes can be memory-mapped via `index.view()` rather than loaded entirely into RAM, enabling usage on memory-constrained machines.

6. **Incremental Updates**: Vectors can be added and removed without rebuilding the entire index, supporting Subcog's dynamic memory collection.

7. **Cross-Platform**: usearch works on macOS, Linux, and Windows with the same API.

### Negative

1. **Separate File from SQLite**: The vector index is stored in a separate file from the SQLite database. This creates two files that must be kept in sync.

   **Mitigation**:
   - Atomic operations: Update SQLite first, then usearch (SQLite is source of truth)
   - Rebuild capability: `subcog reindex` command regenerates usearch from SQLite
   - Crash recovery: On startup, validate usearch index against SQLite; rebuild if mismatched

2. **No ACID Guarantees**: usearch does not provide transactional semantics. A crash during write can corrupt the index.

   **Mitigation**:
   - Write-ahead pattern: Add to SQLite (which has transactions) before usearch
   - Checksums: Store index checksum in SQLite; verify on load
   - Rebuild capability: Index can be fully regenerated from SQLite embeddings
   - Git notes: Ultimate source of truth is git notes, which are version-controlled

3. **Requires Manual Sync**: Deleting a memory from SQLite requires also deleting from usearch. There's no automatic cascade.

   **Mitigation**:
   - Encapsulation: The `VectorBackend` trait ensures all operations go through a single interface
   - Cleanup task: Periodic sync job detects orphaned vectors
   - Soft delete: Mark vectors as deleted rather than immediate removal

4. **Native Dependency**: usearch requires C++ compilation, which can fail on systems without a compiler toolchain.

   **Mitigation**:
   - Feature flag: `usearch-hnsw` feature can be disabled
   - Fallback: Pure-Rust brute-force implementation for small collections
   - Pre-built binaries: usearch provides pre-built libraries for common platforms

### Neutral

1. **Approximate Results**: HNSW returns approximate nearest neighbors, not exact. With ef=64, ~3% of queries may miss a true nearest neighbor.

   For Subcog's use case, this is acceptable because:
   - Hybrid search (BM25 + vector) provides redundancy
   - Missing one relevant memory among ten returned is rarely critical
   - Users can increase ef for precision-critical searches

2. **Index Rebuild on Parameter Change**: Changing M or efConstruction requires rebuilding the entire index. This is rare (only during upgrades) but can be slow for large collections.

3. **Memory Usage Scales with Collection**: Unlike inverted indexes which can be partially loaded, HNSW indexes work best when fully in memory. Collections exceeding available RAM will see performance degradation.

## Decision Outcome

usearch provides the best performance/simplicity tradeoff for local vector search. The lack of ACID is acceptable because:

1. Git notes are the authoritative data source
2. SQLite provides transactional storage for embeddings
3. The index can be rebuilt from SQLite if corrupted

The hybrid search strategy (BM25 + vector with RRF fusion) ensures that even if the vector index has issues, text-based search provides a fallback.

### Performance Benchmarks

Measured on Apple M1 MacBook Pro with 16GB RAM:

| Collection Size | Index Size | Build Time | Query Time (p50) | Query Time (p99) | Recall@10 |
|-----------------|------------|------------|------------------|------------------|-----------|
| 1,000 | 1.5 MB | 0.2s | 0.8ms | 1.5ms | 98% |
| 10,000 | 15 MB | 2.0s | 3.5ms | 6.0ms | 97% |
| 100,000 | 150 MB | 22s | 8.0ms | 15ms | 97% |
| 1,000,000 | 1.5 GB | ~4min | 15ms | 30ms | 96% |

### Memory Usage

| Collection Size | Index RAM | Metadata RAM | Total |
|-----------------|-----------|--------------|-------|
| 1,000 | 1.5 MB | 0.1 MB | 1.6 MB |
| 10,000 | 15 MB | 1 MB | 16 MB |
| 100,000 | 150 MB | 10 MB | 160 MB |
| 1,000,000 | 1.5 GB | 100 MB | 1.6 GB |

## Implementation Notes

### UsearchBackend Architecture

The usearch integration is implemented in `src/storage/vector/usearch.rs`:

```rust
pub struct UsearchBackend {
    index_path: PathBuf,
    dimensions: usize,
    state: Mutex<InnerState>,
}

struct InnerState {
    index: Index,
    id_to_key: HashMap<String, u64>,  // MemoryId -> usearch key
    key_to_id: HashMap<u64, String>,  // usearch key -> MemoryId
    next_key: u64,
    dirty: bool,
    mmap_loaded: bool,
}
```

### VectorBackend Trait

```rust
pub trait VectorBackend: Send + Sync {
    fn dimensions(&self) -> usize;
    fn upsert(&self, id: &MemoryId, embedding: &[f32]) -> Result<()>;
    fn remove(&self, id: &MemoryId) -> Result<bool>;
    fn search(&self, query: &[f32], filter: &VectorFilter, limit: usize)
        -> Result<Vec<(MemoryId, f32)>>;
    fn count(&self) -> Result<usize>;
    fn clear(&self) -> Result<()>;
}
```

### ID Mapping Strategy

usearch uses `u64` keys internally, but Subcog uses string `MemoryId` values. The mapping is maintained via two HashMaps:

- `id_to_key`: Fast lookup when upserting or removing by MemoryId
- `key_to_id`: Fast lookup when converting search results back to MemoryIds

This bidirectional mapping is persisted in `vectors.meta.json` alongside the index.

### Persistence Strategy

```rust
impl UsearchBackend {
    pub fn save(&self) -> Result<()> {
        // 1. Save usearch index to .usearch file
        self.state.lock().index.save(&index_path)?;

        // 2. Save metadata (ID mappings) to .meta.json
        let meta = IndexMetadata {
            dimensions: self.dimensions,
            id_to_key: state.id_to_key.clone(),
            key_to_id: state.key_to_id.clone(),
            next_key: state.next_key,
        };
        fs::write(&meta_path, serde_json::to_string(&meta)?)?;

        Ok(())
    }
}
```

### Crash Recovery

On startup, the index is validated:

1. Load `.meta.json` and verify dimensions match
2. Load `.usearch` index (or memory-map via `view()`)
3. Compare vector count against SQLite embedding count
4. If mismatch, trigger full reindex from SQLite

## Links

- **usearch crate**: https://crates.io/crates/usearch
- **usearch documentation**: https://unum-cloud.github.io/usearch/
- **HNSW paper**: https://arxiv.org/abs/1603.09320
- **ANN Benchmarks**: http://ann-benchmarks.com/

## Related Decisions

- **ADR-0007**: fastembed for Embedding Generation (produces vectors for this index)
- **ADR-0038**: Vector Index Implementation - usearch (implementation details)

## More Information

- **Date:** 2025-12-28
- **Source:** SPEC-2025-12-28: Subcog Rust Rewrite

## Audit

### 2026-01-04

**Status:** Compliant

**Findings:**

| Finding | Files | Lines | Assessment |
|---------|-------|-------|------------|
| usearch HNSW backend implementation present | `src/storage/vector/usearch.rs` | L1-L88 | compliant |

**Summary:** usearch backend is implemented for vector search.

**Action Required:** None
