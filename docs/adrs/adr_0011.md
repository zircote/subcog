---
title: "Hybrid Detection Strategy (Keyword + LLM)"
description: "Combines fast keyword pattern matching with optional LLM classification for search intent detection with graceful fallback."
type: adr
category: search
tags:
  - search-intent
  - hybrid-detection
  - keyword-matching
  - llm-classification
  - graceful-degradation
status: published
created: 2025-12-30
updated: 2026-01-04
author: Claude (Architect)
project: subcog
technologies:
  - llm
audience:
  - developers
  - architects
confidence: high
completeness: complete
---

# ADR-0011: Hybrid Detection Strategy (Keyword + LLM)

## Status

Accepted

## Context

### The Problem Space

Subcog's proactive memory surfacing feature automatically injects relevant memories into AI assistant conversations based on detected user intent. This requires analyzing user prompts to determine what kind of information would be helpful - whether the user is asking "how to" do something, troubleshooting an error, looking for where something is located, or seeking an explanation.

The quality of intent detection directly impacts user experience:
- **False positives** (detecting intent when none exists) waste context window tokens and distract the AI assistant with irrelevant memories
- **False negatives** (missing genuine intent) fail to surface helpful information that could have improved the response
- **Incorrect classification** (detecting the wrong intent type) surfaces memories from inappropriate namespaces, reducing relevance

We need to detect search intent in user prompts to trigger proactive memory surfacing. Two fundamental approaches are available, each with distinct trade-offs:

### Approach 1: Keyword-Based Pattern Matching

Keyword detection uses regular expressions and pattern matching to identify intent signals in user prompts. For example:
- "How do I..." / "How to..." triggers HowTo intent
- "Where is..." / "Where can I find..." triggers Location intent
- "What is the difference between..." triggers Comparison intent
- "error" / "failed" / "not working" triggers Troubleshoot intent

**Characteristics**:
- **Latency**: <10ms (synchronous regex matching)
- **Accuracy**: ~70-80% on well-formed queries, degrades on natural language
- **Determinism**: 100% reproducible given same input
- **Dependencies**: None (pure Rust code)
- **Failure modes**: Cannot fail (always returns a result)

### Approach 2: LLM-Based Classification

LLM classification sends the user prompt to a language model with instructions to classify the intent type and extract relevant topics.

**Characteristics**:
- **Latency**: 50ms-2000ms+ depending on provider and network conditions
- **Accuracy**: ~90-95% across diverse query styles
- **Determinism**: Non-deterministic (LLM outputs vary)
- **Dependencies**: Requires LLM provider (Tier 3 feature per ADR-0003)
- **Failure modes**: Network failures, timeouts, rate limits, provider outages

### The Dilemma

Neither approach alone is satisfactory:

- **Keyword-only** provides fast, reliable baseline but misses nuanced queries. Example: "I'm stuck on this async code" contains troubleshooting intent but no obvious keywords.

- **LLM-only** provides superior accuracy but introduces unacceptable latency and availability risks for a feature that runs on every user message.

### Constraints

1. **Total Hook Latency Budget**: The Claude Code PreToolCall hook must complete in <200ms to avoid perceptible delays in the user experience. This budget must cover intent detection, memory search, and response formatting.

2. **Graceful Degradation**: Per ADR-0003 (Feature Tier System), Tier 3 features must gracefully fall back to lower tiers when LLM providers are unavailable.

3. **Configuration Flexibility**: Users must be able to disable LLM classification entirely for privacy, cost, or latency reasons.

4. **Predictable Behavior**: The system should behave consistently; users should understand why certain memories were surfaced.

## Decision Drivers

### Primary Drivers

1. **Latency Guarantee**: The hook must never block for more than 200ms waiting for LLM classification. Users expect near-instantaneous responses from their AI assistant; adding perceptible delays degrades the experience.

2. **Availability Independence**: Memory surfacing should work even when external LLM providers are unavailable. A network outage should not disable core functionality.

3. **Accuracy Optimization**: When LLM classification is available, it should improve results over keyword-only detection. The hybrid approach should be strictly better than either approach alone.

4. **Resource Efficiency**: LLM API calls have costs (latency, money, rate limits). The system should not waste resources on redundant classification when keywords provide sufficient confidence.

### Secondary Drivers

5. **Debuggability**: Operators should be able to understand which detection source produced a given result, aiding troubleshooting of unexpected behavior.

6. **Testability**: Both detection paths should be independently testable with deterministic inputs and outputs.

7. **Configuration Simplicity**: Users should be able to tune the system without understanding implementation details.

## Decision

We will implement a **hybrid detection strategy** that combines both approaches with the following design:

### Execution Model

1. **Keyword detection runs first and always completes** (<10ms, synchronous)
2. **LLM classification runs in parallel** when configured (async with timeout)
3. **Results are merged** using a weighted algorithm that prefers LLM when available
4. **Keyword result is used as fallback** when LLM times out or fails

### Core Principle: Keyword as Floor, LLM as Ceiling

The keyword detector establishes a baseline (floor) that guarantees some level of intent detection. The LLM classifier can only improve upon this baseline (ceiling) - it cannot make results worse. If LLM fails, we fall back to keyword; we never fall back to "no detection."

## Detection Algorithm Details

### Parallel Execution Model

```
                                 +------------------+
                                 |  User Prompt     |
                                 +--------+---------+
                                          |
                     +--------------------+--------------------+
                     |                                         |
                     v                                         v
         +-----------------------+              +-----------------------------+
         |  Keyword Detection    |              |  LLM Classification         |
         |  (sync, <10ms)        |              |  (async, 200ms timeout)     |
         +-----------------------+              +-----------------------------+
                     |                                         |
                     |  Always completes                       |  May timeout
                     |                                         |
                     v                                         v
         +-----------------------+              +-----------------------------+
         |  KeywordResult {      |              |  Option<LlmResult> {        |
         |    confidence: f32,   |              |    topics: Vec<String>,     |
         |    topics: Vec<...>,  |              |    confidence: f32,         |
         |    patterns: [...],   |              |    reasoning: String,       |
         |  }                    |              |  }                          |
         +-----------------------+              +-----------------------------+
                     |                                         |
                     +--------------------+--------------------+
                                          |
                                          v
                               +---------------------+
                               |  merge_results()   |
                               +---------------------+
                                          |
                                          v
                               +---------------------+
                               |  SearchIntent {    |
                               |    confidence,     |
                               |    topics,         |
                               |    source,         |
                               |  }                 |
                               +---------------------+
```

### Implementation Details

The hybrid detection is implemented in `src/hooks/search_intent/hybrid.rs`:

```rust
/// Detects search intent using hybrid keyword + LLM detection.
///
/// Runs keyword detection immediately and LLM detection in parallel.
/// Merges results with LLM taking precedence for intent type if confidence is high.
pub fn detect_search_intent_hybrid(
    provider: Option<Arc<dyn LlmProviderTrait>>,
    prompt: &str,
    config: &SearchIntentConfig,
) -> SearchIntent {
    // Always run keyword detection (fast, <10ms)
    let keyword_result = detect_search_intent(prompt);

    // If LLM is disabled or no provider, return keyword result
    if !config.use_llm || provider.is_none() {
        return keyword_result.unwrap_or_default();
    }

    // Run LLM classification with timeout
    let timeout = Duration::from_millis(config.llm_timeout_ms);
    let llm_result = run_llm_with_timeout(provider, prompt.to_string(), timeout);

    // Merge results: LLM improves upon keyword baseline
    merge_intent_results(keyword_result, llm_result, config)
}
```

### Confidence Score Calculation

The confidence score determines whether memory surfacing is triggered and how many memories to inject. The merge algorithm combines keyword and LLM confidence with a weighted average:

```rust
/// Merges keyword and LLM detection results into a final confidence score.
/// LLM results boost confidence when available; keyword provides baseline.
fn merge_confidence(
    keyword_confidence: f32,
    llm_result: Option<&LlmResult>,
) -> f32 {
    match llm_result {
        // LLM available: weighted average favoring LLM accuracy
        Some(llm) => {
            let llm_weight = 0.7;
            let keyword_weight = 0.3;
            (llm.confidence * llm_weight) + (keyword_confidence * keyword_weight)
        }
        // LLM unavailable: use keyword confidence with slight penalty
        // (we're less certain without LLM validation)
        None => keyword_confidence * 0.9,
    }
}

/// Minimum confidence threshold to trigger memory surfacing
const CONFIDENCE_THRESHOLD: f32 = 0.6;
```

**Rationale for Weights**:

- **LLM weight (0.7)**: LLM classification is more accurate for nuanced queries and should dominate when available. However, it's not weighted at 1.0 because:
  - LLM outputs are non-deterministic
  - Keyword detection catching something the LLM missed is a signal worth preserving

- **Keyword weight (0.3)**: Keyword detection provides a sanity check. If keywords detect strong intent but LLM disagrees, the averaged result reflects uncertainty.

- **Fallback penalty (0.9x)**: When LLM is unavailable, we reduce keyword confidence slightly because we're missing validation. A keyword confidence of 0.7 becomes 0.63, which may still exceed the 0.6 threshold but with less certainty.

### Topic Merge Strategy

Topics extracted from prompts determine which memories are surfaced. The merge strategy prioritizes LLM-extracted topics (more semantically meaningful) while preserving keyword-extracted topics as fallback:

| Scenario | LLM Topics | Keyword Topics | Final Topics | Confidence Adjustment |
|----------|------------|----------------|--------------|----------------------|
| LLM success, topics match | `["rust", "error"]` | `["rust", "error"]` | `["rust", "error"]` | +0.1 (agreement bonus) |
| LLM success, topics differ | `["async", "tokio"]` | `["rust"]` | `["async", "tokio", "rust"]` | No adjustment |
| LLM success, no keyword topics | `["database"]` | `[]` | `["database"]` | No adjustment |
| LLM timeout | N/A | `["rust", "panic"]` | `["rust", "panic"]` | -0.1 (uncertainty) |
| LLM timeout, high keyword conf | N/A | `["how to", "example"]` | `["how to", "example"]` | No adjustment (conf >= 0.8) |
| Both empty | `[]` | `[]` | `[]` | Force to 0.0 (no intent) |

**Agreement Bonus (+0.1)**: When both detection methods identify the same topics, confidence increases because independent verification suggests higher accuracy.

**Timeout Penalty (-0.1)**: When LLM times out, confidence decreases slightly unless keyword confidence is already high (>=0.8), in which case keywords alone are trusted.

### Intent Type Selection

When both keyword and LLM detection succeed, the system must decide which intent type to use:

```rust
fn merge_intent_results(
    keyword: Option<SearchIntent>,
    llm: Option<SearchIntent>,
    config: &SearchIntentConfig,
) -> SearchIntent {
    match (keyword, llm) {
        // Both available: prefer LLM if high confidence
        (Some(kw), Some(llm_intent)) => {
            if llm_intent.confidence >= config.min_confidence {
                SearchIntent {
                    intent_type: llm_intent.intent_type,  // Use LLM's classification
                    confidence: weighted_average(kw.confidence, llm_intent.confidence),
                    keywords: kw.keywords,                 // Preserve keyword matches
                    topics: prefer_llm_topics(kw.topics, llm_intent.topics),
                    source: DetectionSource::Hybrid,
                }
            } else {
                // LLM confidence too low, fall back to keyword
                SearchIntent { source: DetectionSource::Hybrid, ..kw }
            }
        }
        (Some(kw), None) => kw,  // LLM failed, use keyword
        (None, Some(llm)) => SearchIntent { source: DetectionSource::Llm, ..llm },
        (None, None) => SearchIntent::default(),  // No detection
    }
}
```

### Timeout Behavior

The 200ms timeout is critical for maintaining responsiveness. See [ADR-0014](adr_0014.md) for detailed rationale behind this specific value.

```rust
/// Runs LLM classification with a timeout.
///
/// # Thread Lifecycle
///
/// Spawns a background thread for LLM classification. If timeout is exceeded:
/// - The result is discarded (receiver times out)
/// - The thread continues to completion (Rust cannot kill threads)
/// - Metrics are recorded for monitoring
fn run_llm_with_timeout(
    provider: Option<Arc<dyn LlmProviderTrait>>,
    prompt: String,
    timeout: Duration,
) -> Option<SearchIntent> {
    let provider = provider?;
    let (tx, rx) = mpsc::channel();

    // Spawn LLM classification in background thread
    std::thread::spawn(move || {
        let result = classify_intent_with_llm(provider.as_ref(), &prompt);
        let _ = tx.send(result);  // May fail if receiver dropped (timeout)
    });

    // Wait for result with timeout
    match rx.recv_timeout(timeout) {
        Ok(Ok(intent)) => {
            metrics::counter!("search_intent_llm_completed", "status" => "success")
                .increment(1);
            Some(intent)
        }
        Ok(Err(_)) => {
            metrics::counter!("search_intent_llm_completed", "status" => "error")
                .increment(1);
            None
        }
        Err(mpsc::RecvTimeoutError::Timeout) => {
            // Thread continues in background - this is intentional
            metrics::counter!("search_intent_llm_timeout_total").increment(1);
            tracing::debug!("LLM classification timed out, using keyword-only");
            None
        }
        Err(mpsc::RecvTimeoutError::Disconnected) => {
            metrics::counter!("search_intent_llm_completed", "status" => "disconnected")
                .increment(1);
            None
        }
    }
}
```

**Important**: When the timeout fires, the LLM request continues running in the background. This is a deliberate design choice:
1. Rust cannot safely terminate threads mid-execution
2. Aborting HTTP requests mid-flight can cause resource leaks
3. The background thread will complete and clean up naturally
4. For API-based LLMs, the cost is already incurred

### Thread Safety and Resource Management

The hybrid detection uses `std::thread::spawn` rather than async for LLM calls because:

1. **Blocking LLM SDKs**: Many LLM provider SDKs use blocking I/O internally
2. **Isolation**: A background thread isolates the LLM call from the main async runtime
3. **Simpler Timeout**: `mpsc::recv_timeout` provides straightforward timeout semantics

The spawned thread is not tracked or joined because:
- The thread will complete naturally when the LLM responds
- Memory is released when the thread exits
- Metrics track orphaned threads for monitoring (see CHAOS-H3 resilience testing)

## Considered Options

### Option 1: Keyword Detection Only

**Description**: Use only regex-based pattern matching for intent detection.

**Pros**:
- Zero latency overhead (<10ms)
- No external dependencies
- Deterministic, reproducible results
- Always available (Tier 1)

**Cons**:
- Limited accuracy on natural language queries (~70-80%)
- Cannot understand context or nuance
- Requires maintaining regex patterns for new intents
- Misses queries without explicit keywords

**Example Failure Cases**:
- "I'm stuck on this async code" - no troubleshooting keywords, but clear intent
- "The build keeps failing" - "failing" might be detected, but context is lost
- "Can you explain why this happens?" - "explain" detected, but topic extraction limited

**Verdict**: Provides essential baseline but insufficient for high-quality surfacing.

### Option 2: LLM Classification Only

**Description**: Route all intent detection through an LLM.

**Pros**:
- Highest accuracy (~90-95%)
- Understands context and nuance
- Extracts semantically meaningful topics
- Handles diverse query styles

**Cons**:
- Unacceptable latency (50ms-2000ms+)
- Requires Tier 3 LLM provider
- Non-deterministic outputs
- Fails when provider unavailable
- Cost per classification (API-based providers)

**Verdict**: Cannot be sole detection method due to latency and availability concerns.

### Option 3: Hybrid with Sequential Fallback

**Description**: Try LLM first, fall back to keywords on failure.

**Pros**:
- Simple mental model
- LLM results when available

**Cons**:
- Sequential execution adds latency (LLM timeout + keyword)
- Wastes time waiting for LLM when keywords would suffice
- No benefit from keyword speed when LLM is slow

**Verdict**: Sequential model is strictly worse than parallel execution.

### Option 4: Hybrid with Parallel Execution (Chosen)

**Description**: Run keyword and LLM detection in parallel, merge results.

**Pros**:
- Best of both worlds: keyword speed + LLM accuracy
- Graceful degradation (never worse than keyword-only)
- Parallel execution minimizes latency impact
- LLM improves results when available, doesn't block when unavailable
- Independently testable paths

**Cons**:
- More complex implementation
- Two code paths to maintain
- Potential confusion about which source was used
- Background threads for timed-out LLM calls

**Verdict**: Chosen. Parallel hybrid provides optimal latency/accuracy trade-off.

### Option 5: Cached LLM Results

**Description**: Cache LLM classification results to amortize latency.

**Pros**:
- Reduces repeated LLM calls for similar queries
- Could improve average-case latency

**Cons**:
- Cache invalidation complexity (when do cached intents become stale?)
- Memory overhead for cache storage
- Cold start still has LLM latency
- Query variations defeat cache hits

**Verdict**: Deferred. Could be added later if LLM costs become prohibitive.

## Consequences

### Positive

1. **Best of Both Worlds**: Achieves fast baseline (keyword) with optional accuracy boost (LLM) in a single request/response cycle.

2. **Graceful Degradation**: When LLM is unavailable (network issues, provider outage, user configuration), the system seamlessly falls back to keyword detection without user-visible errors.

3. **Predictable Keyword Behavior**: Keyword detection provides consistent, deterministic behavior that users can learn to expect and work with.

4. **Improved Topic Extraction**: LLM classification significantly improves topic extraction quality, surfacing more relevant memories.

5. **Observable Behavior**: The `DetectionSource` field in `SearchIntent` allows debugging which path produced results, aiding troubleshooting.

6. **Independent Testability**: Keyword and LLM paths can be unit tested independently with deterministic inputs.

### Negative

1. **Implementation Complexity**: The hybrid approach requires more code than either approach alone, including timeout handling, result merging, and metrics.

2. **Two Code Paths**: Both keyword and LLM detection must be maintained, tested, and kept in sync regarding intent types and topic extraction.

3. **Source Confusion**: Users may be confused about why the same query produces different results (LLM non-determinism, timeout variations).

4. **Background Thread Management**: Timed-out LLM calls continue in the background, potentially accumulating if many timeouts occur rapidly.

5. **Debugging Complexity**: When results are unexpected, determining whether the issue is in keyword matching, LLM classification, or merge logic requires investigation.

### Neutral

1. **LLM Configuration**: Users can disable LLM classification entirely via `use_llm: false` in configuration, reducing the system to keyword-only mode.

2. **Detection Source Tracking**: The `SearchIntent.source` field indicates whether the result came from keyword, LLM, or hybrid detection, useful for debugging but adding a field to every result.

3. **Metrics Overhead**: The system records metrics for LLM success/failure/timeout rates, adding observability but also overhead.

## Implementation Notes

### Configuration

The hybrid detection is configured via `SearchIntentConfig`:

```toml
[search_intent]
enabled = true           # Enable/disable intent detection entirely
use_llm = true           # Enable/disable LLM classification
llm_timeout_ms = 200     # Timeout for LLM classification (see ADR-0014)
min_confidence = 0.5     # Minimum confidence to trigger surfacing
```

Environment variable overrides:
- `SUBCOG_SEARCH_INTENT_ENABLED` - Enable/disable detection
- `SUBCOG_SEARCH_INTENT_USE_LLM` - Enable/disable LLM
- `SUBCOG_SEARCH_INTENT_LLM_TIMEOUT_MS` - Timeout in milliseconds
- `SUBCOG_SEARCH_INTENT_MIN_CONFIDENCE` - Confidence threshold

### Metrics

The following metrics are recorded for monitoring:

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `search_intent_llm_started` | Counter | - | LLM classification attempts |
| `search_intent_llm_completed` | Counter | `status` | Completion status (success/error/timeout/disconnected) |
| `search_intent_llm_timeout_total` | Counter | `reason` | Timeout occurrences by reason |
| `search_intent_detection_duration_seconds` | Histogram | `source` | Detection latency by source |

### Testing Strategy

1. **Unit Tests**: Test keyword patterns, LLM mock responses, and merge logic independently
2. **Integration Tests**: Test full hybrid flow with mock LLM provider
3. **Timeout Tests**: Verify graceful degradation when LLM times out
4. **Chaos Tests**: CHAOS-H3 tests verify behavior under LLM failures

## Related Decisions

- [ADR-0003](adr_0003.md): Feature Tier System defines LLM as Tier 3 with graceful degradation
- [ADR-0014](adr_0014.md): 200ms LLM Timeout explains the specific timeout value
- [ADR-0013](adr_0013.md): In-Memory Topic Index for topic-based memory lookup

## More Information

- **Date:** 2025-12-30
- **Source:** SPEC-2025-12-30-001: Proactive Memory Surfacing

## Audit

### 2026-01-04

**Status:** Compliant

**Findings:**

| Finding | Files | Lines | Assessment |
|---------|-------|-------|------------|
| Hybrid intent detection runs keyword + LLM paths | `src/hooks/search_intent/hybrid.rs` | L1-L86 | compliant |
| Parallel execution with timeout | `src/hooks/search_intent/hybrid.rs` | L106-L162 | compliant |
| Result merging with weighted confidence | `src/hooks/search_intent/hybrid.rs` | L164-L209 | compliant |
| Metrics for monitoring | `src/hooks/search_intent/hybrid.rs` | L117, L134-L158 | compliant |

**Summary:** Hybrid detection combines keyword and LLM intent classification with parallel execution, timeout handling, and weighted result merging as specified.

**Action Required:** None
