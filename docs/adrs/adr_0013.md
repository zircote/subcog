---
title: "In-Memory Topic Index"
description: "Use in-memory HashMap with RwLock for topic-to-memory mapping, rebuilt at startup and updated on capture."
type: adr
category: indexing
tags:
  - topic-index
  - in-memory
  - hashmap
  - rwlock
  - caching
status: published
created: 2025-12-30
updated: 2026-01-04
author: Claude (Architect)
project: subcog
technologies:
  - rust
audience:
  - developers
  - architects
confidence: high
completeness: complete
---

# ADR-0013: In-Memory Topic Index

## Status

Accepted

## Context

### The Problem Space

Subcog's proactive memory surfacing feature (see ADR-0011) needs to efficiently retrieve memories by topic. When the hybrid detection system identifies topics like "rust", "error-handling", or "async" in a user prompt, the system must quickly find all memories tagged with or related to those topics.

The MCP server exposes topic-based resources via URIs like `subcog://topics/{topic}`, allowing AI assistants to browse memories by topic. This requires a fast lookup mechanism to map topic names to memory IDs.

### Requirements

1. **Query Latency**: Topic lookups must complete in <5ms to stay within the hook's 200ms budget (which also includes intent detection, memory retrieval, and response formatting)

2. **Update Latency**: Adding new memories to the index must not block the capture path significantly (<10ms overhead acceptable)

3. **Consistency**: The index must accurately reflect the current state of memories (eventual consistency acceptable within seconds)

4. **Memory Efficiency**: Index overhead should be proportional to topic count, not memory count

5. **Startup Time**: Index rebuild should not significantly impact MCP server startup (<200ms for typical projects)

### Data Characteristics

Based on analysis of typical Subcog deployments:

| Metric | Typical Value | 90th Percentile | Notes |
|--------|---------------|-----------------|-------|
| Total memories | 50-200 | 500-1000 | Per project |
| Unique topics | 20-50 | 100-200 | Tags + namespaces + keywords |
| Memories per topic | 5-15 | 30-50 | Average association density |
| Topic name length | 5-20 chars | 30 chars | After normalization |

### Topic Sources

Topics are extracted from three sources per memory:

1. **Explicit Tags**: User-provided tags during capture (e.g., `["rust", "error-handling"]`)
2. **Namespace**: The memory's namespace converted to a topic (e.g., "decisions", "patterns")
3. **Content Keywords**: Top 5 keywords extracted from memory content via TF-IDF-like scoring

## Decision Drivers

### Primary Drivers

1. **Query Performance**: The primary use case is fast topic lookup during memory surfacing. Microsecond-level lookup is achievable with in-memory data structures, while any database query adds milliseconds of overhead.

2. **Implementation Simplicity**: A `HashMap<String, Vec<MemoryId>>` is trivial to implement, test, and reason about. Complex indexing solutions add maintenance burden without proportional benefit for our data scale.

3. **Incremental Updates**: New memories are captured frequently during coding sessions. The index must support efficient incremental updates without full rebuilds.

4. **No Schema Dependencies**: Keeping the topic index separate from the SQLite schema avoids migration complexity for what is essentially derived data.

### Secondary Drivers

5. **Startup Predictability**: Rebuilding from SQLite at startup is deterministic and fast for typical memory counts. Users experience consistent startup times.

6. **Memory Locality**: In-memory data structures benefit from CPU cache locality, especially important for the frequent small lookups during memory surfacing.

7. **Concurrency Model**: Rust's `RwLock` provides an efficient reader-writer lock that allows concurrent reads (common) while serializing writes (rare).

## Decision

We will use an **in-memory HashMap with RwLock**, rebuilt at MCP server startup and updated incrementally on memory capture/delete:

```rust
pub struct TopicIndexService {
    /// Topic to memory ID mappings
    topics: Arc<RwLock<HashMap<String, Vec<MemoryId>>>>,
    /// Topic to namespace mappings (for filtering)
    topic_namespaces: Arc<RwLock<HashMap<String, Vec<Namespace>>>>,
    /// Last refresh timestamp
    last_refresh: Arc<RwLock<Option<Instant>>>,
    /// Refresh interval for periodic rebuilds
    refresh_interval: Duration,
}
```

### Data Structure Choice

**HashMap** was chosen over alternatives for the following reasons:

| Data Structure | Lookup | Insert | Memory | Use Case Fit |
|----------------|--------|--------|--------|--------------|
| `HashMap<String, Vec<MemoryId>>` | O(1) avg | O(1) avg | Moderate | Exact topic match - chosen |
| `BTreeMap<String, Vec<MemoryId>>` | O(log n) | O(log n) | Moderate | Range queries - not needed |
| `Trie` | O(k) | O(k) | High | Prefix matching - overkill |
| SQLite index | O(log n) + I/O | O(log n) + I/O | Low (disk) | Persistence - unnecessary |

For exact topic lookups with ~100 topics, `HashMap` provides optimal O(1) average-case performance with acceptable memory overhead.

### Concurrency Strategy

**RwLock** was chosen over alternatives:

| Primitive | Read Contention | Write Contention | Complexity |
|-----------|-----------------|------------------|------------|
| `RwLock` | None (concurrent) | Blocks reads | Low - chosen |
| `Mutex` | Serialized | Serialized | Lowest |
| `DashMap` | Lock-free reads | Per-shard locks | Medium |
| Crossbeam `SkipList` | Lock-free | Lock-free | High |

The access pattern is heavily read-biased (many topic lookups per memory capture), making `RwLock` ideal. `DashMap` would add dependencies for marginal benefit at our scale.

### Topic Normalization

All topics are normalized before indexing:

```rust
fn normalize_topic(topic: &str) -> String {
    topic
        .trim()
        .to_lowercase()
        .replace(|c: char| !c.is_alphanumeric() && c != '-', "")
}
```

This ensures consistent lookups regardless of input casing or formatting:
- "Rust" -> "rust"
- "  Python  " -> "python"
- "error-handling" -> "error-handling"
- "test_case" -> "testcase"

## Index Operations

### Build (Startup)

At MCP server startup, the index is rebuilt from SQLite:

```rust
pub fn build_index(&self, recall: &RecallService) -> Result<()> {
    let filter = SearchFilter::new();
    let result = recall.list_all(&filter, MAX_INDEX_MEMORIES)?; // 10,000 max

    let mut topics_map: HashMap<String, Vec<MemoryId>> = HashMap::new();
    let mut namespace_map: HashMap<String, Vec<Namespace>> = HashMap::new();

    for hit in &result.memories {
        let memory = &hit.memory;

        // Index from tags
        for tag in &memory.tags {
            let topic = normalize_topic(tag);
            add_topic_entry(&topic, &memory.id, memory.namespace, ...);
        }

        // Index from namespace
        let ns_topic = normalize_topic(memory.namespace.as_str());
        add_topic_entry(&ns_topic, &memory.id, memory.namespace, ...);

        // Index from content keywords (top 5)
        let keywords = extract_content_keywords(&memory.content);
        for keyword in keywords.into_iter().take(5) {
            add_topic_entry(&normalize_topic(&keyword), ...);
        }
    }

    // Deduplicate memory IDs per topic
    for ids in topics_map.values_mut() {
        ids.sort_by(|a, b| a.as_str().cmp(b.as_str()));
        ids.dedup_by(|a, b| a.as_str() == b.as_str());
    }

    // Update index atomically
    *self.topics.write()? = topics_map;
    *self.topic_namespaces.write()? = namespace_map;
    *self.last_refresh.write()? = Some(Instant::now());

    Ok(())
}
```

**Performance**: For 500 memories with ~5 topics each, rebuild takes ~50-100ms.

### Lookup (Query)

Topic lookup returns all memory IDs associated with a topic:

```rust
pub fn get_topic_memories(&self, topic: &str) -> Result<Vec<MemoryId>> {
    let normalized = normalize_topic(topic);
    let guard = self.topics.read()?;
    Ok(guard.get(&normalized).cloned().unwrap_or_default())
}
```

**Performance**: <1ms for any topic, regardless of associated memory count.

### Add (Capture)

When a new memory is captured, the index is updated incrementally:

```rust
pub fn add_memory(
    &self,
    memory_id: &MemoryId,
    tags: &[String],
    namespace: Namespace,
) -> Result<()> {
    let mut topics_guard = self.topics.write()?;
    let mut ns_guard = self.topic_namespaces.write()?;

    // Add from tags
    for tag in tags {
        let topic = normalize_topic(tag);
        topics_guard.entry(topic.clone()).or_default().push(memory_id.clone());
        // ... namespace tracking
    }

    // Add from namespace
    let ns_topic = normalize_topic(namespace.as_str());
    topics_guard.entry(ns_topic).or_default().push(memory_id.clone());

    Ok(())
}
```

**Performance**: <5ms for typical tag counts (3-5 tags per memory).

### Remove (Delete)

When a memory is deleted, it's removed from all topic entries:

```rust
pub fn remove_memory(&self, memory_id: &MemoryId) -> Result<()> {
    let mut topics_guard = self.topics.write()?;

    // Remove from all topic entries
    for ids in topics_guard.values_mut() {
        ids.retain(|id| id.as_str() != memory_id.as_str());
    }

    // Remove empty topics
    topics_guard.retain(|_, ids| !ids.is_empty());

    Ok(())
}
```

**Performance**: O(topics * avg_memories_per_topic), typically <10ms.

## Keyword Extraction

Content keywords are extracted using a simple TF-like scoring:

```rust
fn extract_content_keywords(content: &str) -> Vec<String> {
    // Filter stop words
    static STOP_WORDS: &[&str] = &[
        "the", "a", "an", "is", "are", /* ... 100+ common words */
    ];

    let words: Vec<String> = content
        .split(|c: char| !c.is_alphanumeric())
        .filter(|w| w.len() >= 3 && w.len() <= 30)  // Length bounds
        .map(str::to_lowercase)
        .filter(|w| !STOP_WORDS.contains(&w.as_str()))
        .filter(|w| !w.chars().all(char::is_numeric))  // No pure numbers
        .collect();

    // Count frequencies
    let mut freq: HashMap<String, usize> = HashMap::new();
    for word in words {
        *freq.entry(word).or_insert(0) += 1;
    }

    // Return sorted by frequency
    let mut sorted: Vec<_> = freq.into_iter().collect();
    sorted.sort_by(|a, b| b.1.cmp(&a.1));
    sorted.into_iter().map(|(w, _)| w).collect()
}
```

**Rationale**:
- **Length bounds (3-30)**: Excludes noise (single letters) and unlikely keywords (very long strings)
- **Stop word filtering**: Removes common English words that don't convey topic information
- **Frequency sorting**: Higher-frequency words are more likely to be relevant topics
- **Top 5 limit**: Prevents content-heavy memories from dominating the index

## Considered Options

### Option 1: On-Demand Index Building

Build the topic index lazily on first topic request, then cache.

| Metric | Value |
|--------|-------|
| First Query Latency | 50-200ms (depends on memory count) |
| Subsequent Query Latency | <1ms |
| Memory Usage | Same as chosen |
| Implementation Complexity | Low |

**Analysis**: The first topic query would block while scanning all memories, causing noticeable delays in interactive sessions. For a user asking "what topics do I have?", a 200ms delay is unacceptable.

**Verdict**: Rejected. Unpredictable first-request latency creates poor user experience.

### Option 2: SQLite Persistence

Store the topic index in a dedicated SQLite table with foreign keys to memories.

```sql
CREATE TABLE topic_index (
    topic TEXT NOT NULL,
    memory_id TEXT NOT NULL REFERENCES memories(id),
    namespace TEXT NOT NULL,
    PRIMARY KEY (topic, memory_id)
);
CREATE INDEX idx_topic ON topic_index(topic);
```

| Metric | Value |
|--------|-------|
| First Query Latency | <5ms |
| Subsequent Query Latency | <5ms |
| Memory Usage | Minimal (disk-based) |
| Implementation Complexity | Medium (schema, migrations) |

**Analysis**:
- Requires schema migration infrastructure
- Adds foreign key constraints that complicate memory deletion
- Index rebuild from memories is fast enough (~100ms) that persistence overhead isn't justified
- Topic index is derived data, not primary data - storing it separately creates sync risks

**Verdict**: Rejected. Schema complexity and migration burden outweigh benefits for derived data.

### Option 3: In-Memory HashMap (Chosen)

Maintain a HashMap protected by RwLock, rebuilt at startup.

| Metric | Value |
|--------|-------|
| First Query Latency | <5ms (after startup) |
| Subsequent Query Latency | <1ms |
| Memory Usage | ~50 bytes per topic + 8 bytes per memory reference |
| Implementation Complexity | Low |
| Startup Cost | ~100ms for rebuild |

**Analysis**:
- Fast, predictable query performance
- Simple implementation with Rust standard library
- Rebuild time (~100ms) is negligible compared to MCP server initialization overhead
- Incremental updates avoid full rebuilds during normal operation

**Verdict**: Chosen. Provides optimal performance with minimal complexity.

### Option 4: External Search Service (e.g., Meilisearch)

Use an external search service for topic indexing.

| Metric | Value |
|--------|-------|
| Query Latency | 5-20ms (network I/O) |
| Memory Usage | External (separate process) |
| Implementation Complexity | High (service management) |
| Operational Overhead | High (separate deployment) |

**Analysis**:
- Massive overkill for ~100 topics
- Adds deployment complexity contrary to single-binary goal
- Network latency would dominate query time
- Requires managing separate service lifecycle

**Verdict**: Rejected. External service complexity is unjustified for our scale.

## Memory Overhead Analysis

For a typical project with 200 memories and 50 unique topics:

| Component | Size Calculation | Total |
|-----------|------------------|-------|
| Topic strings | 50 topics * 15 chars avg | ~750 bytes |
| HashMap overhead | 50 entries * 48 bytes | ~2.4 KB |
| Memory ID vectors | 50 topics * 10 memories * 16 bytes | ~8 KB |
| Namespace tracking | 50 topics * 24 bytes | ~1.2 KB |
| **Total** | | **~12 KB** |

For a large project with 1000 memories and 200 topics:

| Component | Size Calculation | Total |
|-----------|------------------|-------|
| Topic strings | 200 topics * 15 chars avg | ~3 KB |
| HashMap overhead | 200 entries * 48 bytes | ~10 KB |
| Memory ID vectors | 200 topics * 25 memories * 16 bytes | ~80 KB |
| Namespace tracking | 200 topics * 24 bytes | ~5 KB |
| **Total** | | **~100 KB** |

This overhead is negligible compared to the memory content itself (typically 1-10MB for a large project).

## Consequences

### Positive

1. **Very Fast Lookups (<5ms)**: In-memory HashMap provides O(1) average-case lookup, easily meeting the <5ms requirement for topic queries.

2. **No Additional Database Schema**: The topic index is entirely runtime state, avoiding schema migrations and SQLite complexity for what is derived data.

3. **Simple Implementation**: Using Rust's standard library (`HashMap`, `RwLock`) results in ~500 lines of straightforward, testable code.

4. **Automatic Refresh on Capture**: Incremental updates on memory capture keep the index current without periodic rebuilds or cache invalidation complexity.

5. **Predictable Startup**: Index rebuild during MCP server startup is fast (~100ms) and deterministic, providing consistent user experience.

### Negative

1. **Lost on Server Restart**: The index exists only in memory and must be rebuilt from SQLite on every MCP server restart. This adds ~100ms to startup time.

2. **Memory Overhead for Large Projects**: Projects with thousands of memories and hundreds of topics consume more RAM. However, overhead is <100KB even for large projects.

3. **RwLock Contention on Updates**: Write operations (memory capture/delete) acquire exclusive locks, briefly blocking concurrent reads. With our write frequency (<1/second typical), this is not a practical concern.

4. **Potential Inconsistency Window**: Between memory capture and index update, there's a brief window where the index doesn't reflect the new memory. This is acceptable for our eventual-consistency model.

### Neutral

1. **Async Rebuild at Startup**: Index rebuild runs asynchronously during MCP server initialization, not blocking the startup sequence.

2. **Typical Project Scale**: The ~100 topics typical for projects fits entirely in L2 cache, maximizing lookup performance.

3. **Periodic Refresh Option**: The `refresh_interval` configuration allows periodic full rebuilds to correct any drift from incremental updates.

## Implementation Notes

### Thread Safety

The `TopicIndexService` is designed for concurrent access:

```rust
impl TopicIndexService {
    // Multiple concurrent reads allowed
    pub fn get_topic_memories(&self, topic: &str) -> Result<Vec<MemoryId>> {
        let guard = self.topics.read()?;  // Read lock
        // ...
    }

    // Writes are serialized
    pub fn add_memory(&self, ...) -> Result<()> {
        let mut guard = self.topics.write()?;  // Write lock
        // ...
    }
}
```

The `Arc<RwLock<_>>` pattern allows the service to be shared across async tasks safely.

### Error Handling

Lock poisoning is handled gracefully:

```rust
let guard = self.topics.read().map_err(|_| Error::OperationFailed {
    operation: "get_topic_memories".to_string(),
    cause: "Lock poisoned".to_string(),
})?;
```

If a thread panics while holding the lock, subsequent operations return an error rather than panicking. In practice, this is rare and indicates a bug elsewhere.

### Refresh Strategy

The index supports both incremental updates and periodic full rebuilds:

```rust
// Incremental: called on memory capture
topic_index.add_memory(&memory_id, &tags, namespace)?;

// Full rebuild: called periodically or on demand
if topic_index.needs_refresh() {
    topic_index.build_index(&recall_service)?;
}
```

The default refresh interval is 5 minutes, providing a safety net for any incremental update bugs.

## Related Decisions

- [ADR-0011](adr_0011.md): Hybrid Detection Strategy uses topic index for memory surfacing
- [ADR-0014](adr_0014.md): 200ms LLM Timeout constrains total lookup budget

## More Information

- **Date:** 2025-12-30
- **Source:** SPEC-2025-12-30-001: Proactive Memory Surfacing

## Audit

### 2026-01-04

**Status:** Compliant

**Findings:**

| Finding | Files | Lines | Assessment |
|---------|-------|-------|------------|
| In-memory topic index uses Arc<RwLock<HashMap>> | `src/services/topic_index.rs` | L35-L56 | compliant |
| Startup rebuild from RecallService | `src/services/topic_index.rs` | L86-L172 | compliant |
| Incremental add/remove/update operations | `src/services/topic_index.rs` | L261-L366 | compliant |
| Topic normalization | `src/services/topic_index.rs` | L523-L528 | compliant |
| Keyword extraction with stop words | `src/services/topic_index.rs` | L530-L564 | compliant |

**Summary:** Topic index is maintained in-memory with RwLock-protected HashMaps, supporting both full rebuilds and incremental updates as specified.

**Action Required:** None
