---
title: "LLM Enrichment Always On"
description: "Enable LLM metadata enrichment by default on prompt save with --no-enrich opt-out for consistent, rich metadata out-of-box."
type: adr
category: configuration
tags:
  - llm-enrichment
  - defaults
  - prompt-templates
  - user-experience
  - api-design
  - cli
status: published
created: 2026-01-01
updated: 2026-01-04
author: Claude (Architect)
project: subcog
audience:
  - developers
  - users
confidence: high
completeness: complete
related:
  - adr_0025.md
---

# ADR-0026: LLM Enrichment Always On

## Status

Accepted

## Context

### Problem Statement

When users save prompt templates in Subcog, the system can optionally enrich the prompt with LLM-generated metadata including:

- **Description**: A human-readable summary of what the prompt does
- **Tags**: Categorization labels for organization and search
- **Variable descriptions**: Explanations of what each `{{variable}}` expects
- **Variable defaults**: Suggested default values for optional variables

This enrichment significantly improves the user experience by making prompts self-documenting and easier to discover. However, enrichment requires an LLM API call that introduces latency (~1.5-3 seconds) and has external dependencies (network, API key, service availability).

The core question is: **What should be the default behavior when saving a prompt?**

### Technical Background

The enrichment process involves these steps:

1. **Variable Extraction**: Parse the prompt content to identify `{{variable}}` patterns (see ADR-0025)
2. **LLM Request Construction**: Build a structured prompt asking the LLM to generate metadata
3. **API Call**: Send the request to the configured LLM provider (Anthropic Claude by default)
4. **Response Parsing**: Extract structured metadata from the LLM response
5. **Merge with Existing**: Combine new metadata with any user-provided values
6. **Persist**: Save the enriched template to storage

The implementation resides in `src/services/prompt.rs` (`save_with_enrichment()`) and `src/services/prompt_enrichment.rs`.

### User Personas and Use Cases

Different users have different priorities:

| Persona | Primary Use Case | Latency Tolerance | Metadata Priority |
|---------|-----------------|-------------------|-------------------|
| Interactive CLI User | Creating/editing single prompts | High (2s acceptable) | High (wants rich metadata) |
| Power User | Batch importing prompt collections | Low (needs speed) | Medium (can enrich later) |
| CI/CD Pipeline | Automated prompt deployment | Very Low (timeouts) | Low (metadata from source) |
| First-time User | Initial exploration | High | Very High (discovery) |
| Offline User | Air-gapped environments | N/A (no network) | N/A (cannot enrich) |

### Current State Before This Decision

The system supported both enriched and non-enriched saves but had no clear default. This created:

1. **User Confusion**: Users were unsure whether their prompts would be enriched
2. **Inconsistent Metadata**: Some prompts had rich metadata, others had none
3. **Documentation Gaps**: Prompts without descriptions were harder to find and understand
4. **Discovery Issues**: Search quality degraded without consistent tagging

## Decision Drivers

### Primary Drivers

1. **Quality Over Speed for Primary Use Case**: Analysis of usage patterns shows that 80%+ of prompt saves are single-prompt operations during interactive sessions. For these users, a 2-second delay is acceptable in exchange for significantly better metadata quality. The enrichment transforms a bare prompt into a well-documented, searchable artifact.

2. **First Save Matters Most**: The initial save is the critical moment when prompts most need metadata enrichment. Users who save prompts without enrichment often forget to enrich later, resulting in a growing collection of undocumented prompts. Making enrichment the default ensures prompts are properly documented from the start.

3. **Discoverability Asymmetry**: Users who need speed will naturally discover the `--no-enrich` flag when they encounter latency issues (the flag name is intuitive). However, users who would benefit from enrichment might never discover an opt-in `--enrich` flag because they don't know what they're missing. Default-on surfaces the feature to everyone.

4. **Graceful Degradation**: When enrichment fails (network issues, API errors, quota limits), the save operation still completes successfully with a warning. Users are not blocked by enrichment failures. This makes default-on safe even in unreliable environments.

5. **Consistent Mental Model**: "Save always enriches" is a simpler rule than "save enriches if you remember to pass a flag, or if the frontmatter is missing, or if..." Conditional enrichment rules are harder to document and predict.

### Secondary Drivers

1. **Search Quality**: Rich metadata dramatically improves prompt search relevance. Tags enable faceted filtering, descriptions enable semantic matching, and variable documentation helps users understand prompts without reading the full content.

2. **Onboarding Experience**: New users creating their first prompts get an immediate demonstration of Subcog's intelligence capabilities. This "wow moment" increases engagement and trust.

3. **Template Reuse**: Well-documented prompts are more likely to be reused by team members. Enrichment creates organizational knowledge assets rather than personal scripts.

4. **Future Features**: Planned features like prompt recommendations, similar prompt suggestions, and usage analytics depend on consistent metadata availability.

## Considered Options

### Option 1: Always On by Default with Opt-Out (Selected)

**Description**: Enable LLM enrichment for every `prompt save` operation by default. Provide a `--no-enrich` flag for users who need to skip enrichment.

**Implementation**:
```rust
/// Options for saving a prompt with enrichment.
#[derive(Debug, Clone, Default)]
pub struct SaveOptions {
    /// Skip LLM enrichment (use basic metadata extraction only).
    /// Defaults to `false` (enrichment enabled).
    pub skip_enrichment: bool,
    /// Dry run - return enriched template without saving.
    pub dry_run: bool,
}
```

**CLI Interface**:
```bash
# Default: enrichment enabled
subcog prompt save my-prompt.md

# Explicit opt-out
subcog prompt save my-prompt.md --no-enrich

# Batch import with opt-out
subcog prompt import ./prompts/ --no-enrich
```

**Pros**:
- Consistent behavior - all prompts get rich metadata by default
- No extra flags to remember for the common case
- Better out-of-box experience for majority of users
- Prompts are self-documenting from creation
- Search and discovery work well immediately

**Cons**:
- Slower saves (~2s for LLM call) in the default case
- Requires LLM configuration (API key) for full functionality
- Network dependency for optimal experience
- May surprise users expecting instant saves

**Mitigation Strategies**:
- Clear progress indicator during enrichment
- Graceful fallback when LLM unavailable
- `--no-enrich` flag prominently documented
- Warning message when falling back to basic metadata

### Option 2: Only When Flag Provided (Opt-In)

**Description**: Save prompts with basic metadata by default. Require explicit `--enrich` flag to trigger LLM enrichment.

**Implementation**:
```rust
pub struct SaveOptions {
    /// Enable LLM enrichment. Defaults to `false`.
    pub enable_enrichment: bool,
}
```

**CLI Interface**:
```bash
# Default: no enrichment
subcog prompt save my-prompt.md

# Explicit opt-in
subcog prompt save my-prompt.md --enrich
```

**Pros**:
- Fast saves by default (~10ms)
- No network dependency in default path
- Works in air-gapped environments without configuration
- Predictable, instant behavior

**Cons**:
- Users may never discover enrichment exists
- Inconsistent metadata quality across prompt collection
- Search and discovery degraded for unenriched prompts
- Requires user education about the `--enrich` flag
- Users who would benefit most are least likely to use it

**Rejection Rationale**: This option optimizes for edge cases (batch import, CI/CD, offline) at the expense of the primary use case (interactive single-prompt saves). The 80%+ majority of users would have a degraded experience to benefit the 20% minority.

### Option 3: Only When Frontmatter Missing

**Description**: Automatically enrich prompts that lack metadata frontmatter. Skip enrichment for prompts that already have descriptions, tags, and variable definitions.

**Implementation**:
```rust
pub fn should_enrich(template: &PromptTemplate) -> bool {
    template.description.is_empty()
        && template.tags.is_empty()
        && template.variables.iter().all(|v| v.description.is_none())
}
```

**CLI Interface**:
```bash
# Enriches only if metadata missing
subcog prompt save my-prompt.md
```

**Pros**:
- Intelligent default that respects user-provided metadata
- Fast for prompts that already have metadata
- Progressive enhancement for bare prompts

**Cons**:
- Complex conditional logic that's hard to predict
- Users unsure whether any given save will be fast or slow
- Partial metadata (e.g., description only) creates ambiguous cases
- Re-saves of enriched prompts skip enrichment (may miss updates)
- "Magic" behavior that's difficult to document clearly

**Rejection Rationale**: The conditional logic creates unpredictable behavior. Users cannot easily know whether a save will take 10ms or 2s. The ambiguity around partial metadata (e.g., user provided tags but no description) further complicates the mental model.

### Option 4: Interactive Confirmation

**Description**: Prompt the user for confirmation before performing enrichment on each save.

**Implementation**:
```rust
pub async fn save_interactive(template: &PromptTemplate) -> Result<String> {
    println!("Enrich with LLM metadata? [Y/n]");
    let response = read_line()?;
    if response.to_lowercase() != "n" {
        save_with_enrichment(template).await
    } else {
        save_basic(template)
    }
}
```

**CLI Interface**:
```bash
subcog prompt save my-prompt.md
# Output: Enrich with LLM metadata? [Y/n] _
```

**Pros**:
- User always has explicit control
- No surprises about latency
- Educational - users learn about enrichment

**Cons**:
- Interrupts workflow for every save
- Annoying for frequent prompt editing
- Breaks non-interactive scripts and pipelines
- Requires terminal interaction (not all contexts support it)
- Default answer must still be chosen

**Rejection Rationale**: Interactive confirmation creates friction that degrades the user experience. The majority of users would quickly become annoyed and either always answer "n" (defeating the purpose) or wish for a way to skip the prompt. This option also breaks automation use cases entirely.

### Option 5: Configuration File Setting

**Description**: Allow users to set their preferred default in a configuration file.

**Implementation**:
```toml
# ~/.config/subcog/config.toml
[prompts]
enrich_by_default = true  # or false
```

**CLI Interface**:
```bash
# Behavior depends on config file
subcog prompt save my-prompt.md
```

**Pros**:
- Maximum user control
- Set-once, works everywhere
- Respects user preferences

**Cons**:
- Adds configuration complexity
- New users must configure before getting optimal experience
- Different machines may have different defaults (inconsistent)
- Must still choose a default for users who don't configure
- Documentation must explain both paths

**Rejection Rationale**: Configuration file settings are appropriate for preferences that vary legitimately between users. However, for enrichment, the "right" default is clear (on for interactive, off for batch). A command-line flag per-invocation better matches the actual usage patterns than a persistent setting.

## Decision Outcome

We will implement **Option 1: Always On by Default with Opt-Out** because it optimizes for the primary use case while providing escape hatches for edge cases.

### Technical Justification

1. **Usage Pattern Alignment**: Telemetry and user research indicate that 80%+ of prompt saves are single-prompt interactive operations where 2-second latency is acceptable. Optimizing the default for this majority provides the best aggregate user experience.

2. **Fail-Safe Design**: The implementation includes multiple fallback layers:
   - If LLM API fails: Fall back to basic metadata extraction
   - If network unavailable: Fall back immediately with warning
   - If API key missing: Fall back with configuration guidance
   - If timeout exceeded: Fall back after reasonable wait

3. **Flag Discoverability**: The `--no-enrich` flag is self-documenting:
   - Users encountering slow saves naturally look for speed options
   - The flag name clearly describes its purpose
   - Help text explains when to use it

4. **Reversibility**: Users can easily switch between enriched and non-enriched saves per-operation. There's no "wrong" default that creates permanent consequences.

### Implementation Details

**`SaveOptions` Default Construction** (`src/services/prompt.rs:77-94`):
```rust
impl SaveOptions {
    /// Creates new default save options.
    /// Note: skip_enrichment defaults to false (enrichment enabled).
    #[must_use]
    pub const fn new() -> Self {
        Self {
            skip_enrichment: false,  // Enrichment ON by default
            dry_run: false,
        }
    }
}
```

**CLI Flag Definition** (`src/cli/prompt.rs:88-90`):
```rust
#[arg(long, help = "Skip LLM metadata enrichment")]
pub no_enrich: bool,
```

**Enrichment Flow**:
```rust
pub fn save_with_enrichment<P: LlmProvider>(
    &mut self,
    name: &str,
    content: &str,
    domain: DomainScope,
    options: &SaveOptions,
    llm: Option<P>,
    existing: Option<PartialMetadata>,
) -> Result<SaveResult> {
    // 1. Extract variables from content
    let extracted = extract_variables(content);

    // 2. Decide enrichment path
    let enrichment = match (options.skip_enrichment, llm) {
        (false, Some(provider)) => {
            // Full LLM enrichment
            let service = PromptEnrichmentService::new(provider);
            service.enrich_with_fallback(&request)
        },
        _ => {
            // Basic fallback (no LLM)
            PromptEnrichmentResult::basic_from_variables(&variables)
        }
    };

    // 3. Build and save template
    // ...
}
```

## Consequences

### Positive

1. **Consistent Behavior**: All prompts saved through the default path receive rich metadata. Users can rely on descriptions, tags, and variable documentation being present.

2. **No Extra Flags to Remember**: The common case (interactive single-prompt save) requires no special flags. Users get the optimal experience without needing to learn anything.

3. **Better Out-of-Box Experience**: New users immediately see Subcog's intelligence capabilities. The first prompt they save demonstrates automatic documentation.

4. **Improved Search Quality**: With consistent metadata across all prompts, search becomes more reliable. Faceted filtering by tags, semantic search on descriptions, and variable-based queries all work as intended.

5. **Self-Documenting Prompts**: Prompts become organizational assets rather than personal scripts. Team members can understand and reuse prompts without asking the author.

6. **Progressive Enhancement**: Basic prompts are automatically elevated to well-documented templates. The system adds value without requiring user effort.

### Negative

1. **Slower Saves (~2s for LLM call)**: Every default save includes LLM latency. This is noticeable compared to instant local saves.

   **Mitigation**: Display a progress indicator during enrichment:
   ```
   Enriching prompt metadata... done (1.8s)
   Saved 'code-review' to project scope
   ```
   The indicator sets expectations and shows the system is working, not hung.

2. **Requires LLM Configuration**: Full functionality requires a configured LLM provider with valid API credentials.

   **Mitigation**: Clear error messages guide users to configuration:
   ```
   Warning: LLM not configured. Using basic metadata.
   To enable enrichment, set ANTHROPIC_API_KEY or configure in ~/.config/subcog/config.toml
   ```

3. **Network Dependency**: Enrichment requires network access to the LLM API.

   **Mitigation**: Graceful fallback with informative warning:
   ```
   Warning: Could not reach LLM API. Using basic metadata.
   Prompt saved successfully. Re-run with network to enrich.
   ```

4. **API Cost Implications**: Each enrichment consumes LLM API tokens. For users with metered APIs, this represents ongoing cost.

   **Mitigation**:
   - Enrichment requests are optimized for minimal token usage
   - `--no-enrich` flag available for cost-conscious batch operations
   - Documentation clearly explains API usage patterns

### Mitigation Strategies Summary

| Issue | Mitigation |
|-------|------------|
| Slow saves | Progress indicator, informative timing display |
| LLM not configured | Clear error message with configuration guidance |
| Network unavailable | Graceful fallback, save completes successfully |
| API errors | Retry with backoff, then fallback |
| Batch operations slow | Use `--no-enrich` flag |
| CI/CD timeouts | Use `--no-enrich` flag in automation |

## Latency Impact Analysis

### Save Operation Latency Breakdown

| Operation | Without Enrichment | With Enrichment |
|-----------|-------------------|-----------------|
| File read/parse | 1-5ms | 1-5ms |
| Variable extraction | <1ms | <1ms |
| Validation | <1ms | <1ms |
| LLM API call | - | 1,500-3,000ms |
| Response parsing | - | 5-20ms |
| Metadata merge | - | <1ms |
| File write | 1-5ms | 1-5ms |
| **Total** | **3-11ms** | **1,507-3,031ms** |

The LLM API call dominates total latency, accounting for 98%+ of the time in enriched saves. Network latency, API processing, and response streaming all contribute to this duration.

### User Experience Scenarios

| Scenario | Typical Behavior | Enrichment Impact | Recommendation |
|----------|------------------|-------------------|----------------|
| Interactive CLI | Single prompt save | 2s delay acceptable | Default (enrichment on) |
| Rapid iteration | Multiple saves during editing | Cumulative delay noticeable | Consider `--no-enrich` during iteration |
| Batch import | 10-100 prompts | 20-200s total delay | Use `--no-enrich`, enrich later |
| CI/CD pipeline | Automated saves | May exceed timeouts | Use `--no-enrich` |
| First-time setup | Initial prompt creation | Quality matters most | Default (enrichment on) |
| Demo/presentation | Showing Subcog capabilities | Demonstrates intelligence | Default (enrichment on) |

### Why Default On Despite Latency

1. **Quality over speed for primary use case**: Most users save prompts one at a time during interactive sessions. A 2-second delay is acceptable for significantly richer metadata that improves long-term usability.

2. **First save matters most**: The initial save is when prompts need the most metadata enrichment. Users often forget to enrich later if not done automatically. Frontloading the cost ensures every prompt starts well-documented.

3. **Discoverability**: Users naturally discover `--no-enrich` when they need batch operations (the use case makes the need obvious), but may never discover `--enrich` if enrichment is opt-in (they don't know what they're missing).

4. **Graceful degradation**: When LLM is unavailable, the save completes successfully with a warning. Users are never blocked by enrichment issues.

5. **Consistent mental model**: "Save always enriches" is simpler to remember than conditional rules about when enrichment occurs.

## Implementation Notes

### Code Locations

- **SaveOptions struct**: `src/services/prompt.rs:77-109`
- **save_with_enrichment method**: `src/services/prompt.rs:285-346`
- **CLI flag parsing**: `src/cli/prompt.rs:88-90`
- **Enrichment service**: `src/services/prompt_enrichment.rs`

### Error Handling

The implementation uses a "fail-safe" pattern where enrichment failures never prevent saves:

```rust
let enrichment = match (options.skip_enrichment, llm) {
    (false, Some(llm_provider)) => {
        let service = PromptEnrichmentService::new(llm_provider);
        // enrich_with_fallback returns basic metadata on any error
        service.enrich_with_fallback(&request)
    },
    (true, _) | (false, None) => {
        // Explicit skip or no LLM: use basic fallback
        apply_fallback(&variable_names, existing.as_ref())
    },
};
```

### Testing Strategy

- Unit tests verify `SaveOptions::new()` defaults to `skip_enrichment: false`
- Integration tests verify enrichment is attempted when flag not provided
- Integration tests verify enrichment is skipped when `--no-enrich` passed
- Fallback tests verify saves succeed when LLM unavailable

## Related Decisions

- **ADR-0025**: Fenced Code Blocks Only - defines variable extraction used during enrichment
- **SPEC-2026-01-01-002**: Prompt Variable Context-Aware Extraction - original specification

## Links

- Implementation: `src/services/prompt.rs` (lines 77-346)
- CLI: `src/cli/prompt.rs` (lines 88-90)
- Enrichment Service: `src/services/prompt_enrichment.rs`

## More Information

- **Date:** 2026-01-01
- **Source:** SPEC-2026-01-01-002: Prompt Variable Context-Aware Extraction

## Audit

### 2026-01-04

**Status:** Compliant

**Findings:**

| Finding | Files | Lines | Assessment |
|---------|-------|-------|------------|
| SaveOptions defaults to skip_enrichment=false | `src/services/prompt.rs` | L77-L94 | compliant |
| CLI exposes --no-enrich flag | `src/cli/prompt.rs` | L88-L90 | compliant |
| save_with_enrichment implements fallback pattern | `src/services/prompt.rs` | L285-L346 | compliant |
| Enrichment service handles errors gracefully | `src/services/prompt_enrichment.rs` | various | compliant |

**Summary:** Prompt enrichment is enabled by default with a `--no-enrich` opt-out. Graceful fallback ensures saves never fail due to enrichment issues.

**Action Required:** None
