---
title: "Existing LLM Provider Infrastructure"
description: "Reuse existing LlmProvider trait from src/llm/mod.rs for enrichment to leverage proven multi-provider infrastructure."
type: adr
category: architecture
tags:
  - llm-provider
  - infrastructure
  - reuse
  - abstraction
status: accepted
created: 2026-01-01
updated: 2026-01-04
author: Claude (Architect)
project: subcog
technologies:
  - rust
  - anthropic
  - openai
  - ollama
audience:
  - developers
  - architects
confidence: high
completeness: complete
---

# ADR-0029: Existing LLM Provider Infrastructure

## Status

Accepted

## Context

How to implement LLM calls for enrichment?

- Use existing `LlmProvider` trait
- Implement new MCP sampling handler
- Direct API calls

## Decision Drivers

### Primary Decision Drivers

1. **Code Reuse**: The `LlmProvider` trait is already implemented and tested for multiple providers. Reusing it avoids duplicating this complexity.

2. **Multi-Provider Support**: The existing infrastructure supports Anthropic, OpenAI, Ollama, and LM Studio. Enrichment should work with any configured provider.

3. **Established Patterns**: Error handling, retries, and configuration patterns are already established in the LLM infrastructure.

### Secondary Decision Drivers

1. **Reduced Implementation Effort**: Using existing infrastructure significantly reduces the work needed to implement enrichment.

2. **Consistency**: Users configure LLM providers once; enrichment uses the same configuration.

## Decision

We will use the **existing `LlmProvider` trait** from `src/llm/mod.rs`.

## Consequences

### Positive

- Reuses proven infrastructure
- Supports multiple providers (Anthropic, OpenAI, Ollama, LM Studio)
- Configuration already exists
- Error handling patterns established

### Negative

- MCP sampling remains unimplemented (declared but no handler)
- Tighter coupling to local LLM config

### Neutral

- MCP sampling implementation is out of scope for this spec

## Considered Options

### Option 1: Use Existing LlmProvider Trait (Selected)

**Description**: Leverage the existing `LlmProvider` trait and its implementations.

**Advantages**: Proven infrastructure, multi-provider support, established error handling.

**Disadvantages**: Coupling to local LLM configuration.

### Option 2: Implement New MCP Sampling Handler

**Description**: Use MCP protocol's sampling capability for LLM calls.

**Advantages**: Protocol-standard approach, potential for remote LLM access.

**Disadvantages**: Not yet implemented, adds complexity, out of scope.

### Option 3: Direct API Calls

**Description**: Make direct HTTP calls to LLM APIs without abstraction.

**Advantages**: Simple, no abstraction overhead.

**Disadvantages**: Duplicates existing provider logic, no multi-provider support.

**Note:** MCP sampling implementation is out of scope for this spec.

## More Information

- **Date:** 2026-01-01
- **Source:** SPEC-2026-01-01-002: Prompt Variable Context-Aware Extraction

## Audit

### 2026-01-04

**Status:** Compliant

**Findings:**

| Finding | Files | Lines | Assessment |
|---------|-------|-------|------------|
| LlmProvider trait defines provider interface | `src/llm/mod.rs` | L180-L243 | compliant |

**Summary:** LLM provider trait is the central abstraction used by services.

**Action Required:** None
