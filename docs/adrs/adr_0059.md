---
title: "Central Event Bus for Memory Events"
description: "Use Tokio broadcast channel with a shared event bus for memory lifecycle events, decoupling producers from consumers."
type: adr
category: architecture
tags:
  - event-bus
  - tokio
  - broadcast
  - observability
  - audit
status: accepted
created: 2026-01-04
updated: 2026-01-04
author: Claude (Architect)
project: subcog
technologies:
  - rust
  - tokio
audience:
  - developers
  - architects
confidence: high
completeness: complete
---

# ADR-0059: Central Event Bus for Memory Events

## Status

Accepted

## Context

### Problem Statement

Subcog emits events throughout the memory lifecycle: capture, recall, update, archive, delete, sync, consolidation, and garbage collection. These events need to be consumed by multiple subsystems for different purposes:

1. **Audit Logging**: SOC2/GDPR compliance requires immutable records of all memory operations with HMAC chain integrity
2. **Metrics Collection**: Prometheus counters and histograms for operational monitoring
3. **Distributed Tracing**: OpenTelemetry spans for request flow visualization
4. **Webhook Notifications**: Real-time HTTP callbacks to external systems
5. **Future Consumers**: Analytics, replication, debugging tools

The naive approach of directly calling each consumer from each producer creates tight coupling that makes the system brittle and difficult to extend.

### Architectural Challenge

Consider the memory capture flow without an event bus:

```rust
// Tightly coupled approach (problematic)
impl CaptureService {
    pub fn capture(&self, memory: Memory) -> Result<MemoryId> {
        let id = self.storage.insert(&memory)?;

        // Direct coupling to every consumer
        self.audit_logger.log_capture(&id, &memory)?;
        self.metrics.increment_capture_counter()?;
        self.tracer.record_capture_span(&id)?;
        self.webhook_dispatcher.notify_capture(&id, &memory)?;
        // ... every new consumer requires modification here

        Ok(id)
    }
}
```

This approach has severe drawbacks:

1. **Modification Cascade**: Adding a new consumer requires modifying every producer
2. **Dependency Explosion**: CaptureService depends on audit, metrics, tracing, webhooks, etc.
3. **Testing Complexity**: Unit tests must mock all consumers even when testing capture logic
4. **Failure Coupling**: If webhook dispatch fails, should capture fail? Unclear semantics.
5. **Circular Dependencies**: Audit logger might need to emit events, creating cycles

### Event-Driven Alternative

An event bus decouples producers from consumers:

```rust
// Decoupled approach (event bus)
impl CaptureService {
    pub fn capture(&self, memory: Memory) -> Result<MemoryId> {
        let id = self.storage.insert(&memory)?;

        // Single publish point, consumers subscribe independently
        self.event_bus.publish(MemoryEvent::Captured {
            meta: EventMeta::new("capture", correlation_id),
            memory_id: id.clone(),
            namespace: memory.namespace,
            domain: memory.domain,
            content_length: memory.content.len(),
        });

        Ok(id)
    }
}
```

Consumers subscribe independently:

```rust
// Audit logger subscribes to all events
let mut receiver = event_bus.subscribe();
tokio::spawn(async move {
    while let Ok(event) = receiver.recv().await {
        audit_logger.log_event(&event).await;
    }
});

// Metrics collector subscribes with filtering
let mut receiver = event_bus.subscribe_event_type("captured");
tokio::spawn(async move {
    while let Ok(event) = receiver.recv().await {
        metrics::counter!("memory_captured_total").increment(1);
    }
});
```

### Tokio Broadcast Channel Characteristics

Tokio's `broadcast` channel is specifically designed for this use case:

| Characteristic | Behavior | Implication for Subcog |
|---------------|----------|------------------------|
| **Multi-producer** | Any number of senders | All services can publish events |
| **Multi-consumer** | Any number of receivers | Audit, metrics, webhooks all subscribe |
| **Bounded buffer** | Fixed capacity (default 1024) | Must handle overflow gracefully |
| **Lagging receivers** | Slow receivers get `RecvError::Lagged(n)` | Monitor and alert on lag |
| **Clone semantics** | Events must be `Clone` | `MemoryEvent` implements Clone |
| **Ordering** | FIFO per sender | Correlation IDs maintain causality |

## Decision Drivers

### Primary Decision Drivers

1. **Loose Coupling (Weight: 35%)**
   - Producers should not know about consumers
   - Adding new consumers should not require producer changes
   - Removing consumers should not affect producers
   - Clear separation of concerns

2. **Multiple Subscriber Support (Weight: 25%)**
   - Audit logging (compliance requirement)
   - Metrics collection (operational requirement)
   - Webhook dispatch (integration requirement)
   - Future subscribers (extensibility)

3. **Observability Evolution (Weight: 20%)**
   - New observability consumers can be added without modifying core logic
   - Sampling strategies can change per-consumer
   - Security-sensitive events can be routed separately

4. **Lightweight Implementation (Weight: 20%)**
   - No external message broker dependency
   - No network latency for event delivery
   - Single-process deployment model
   - Minimal resource overhead

### Secondary Decision Drivers

5. **Testability**
   - Producers testable without consumers
   - Consumers testable with synthetic events
   - Event flow traceable via correlation IDs

6. **Failure Isolation**
   - Consumer failures do not affect producers
   - Slow consumers do not block producers
   - Graceful degradation under load

7. **Debugging Support**
   - Events carry correlation IDs for tracing
   - Event metadata includes source component
   - Timestamps enable timeline reconstruction

## Considered Options

### Option 1: Direct Logging from Each Producer

**Description**: Each producer directly calls logging/metrics/audit functions without abstraction.

**Implementation**:
```rust
impl CaptureService {
    pub fn capture(&self, memory: Memory) -> Result<MemoryId> {
        let id = self.storage.insert(&memory)?;
        tracing::info!(memory_id = %id, "Memory captured");
        metrics::counter!("capture_total").increment(1);
        self.audit.log_capture(&id)?;
        Ok(id)
    }
}
```

**Advantages**:
- Simple, no abstraction layer
- Direct control over what gets logged
- No event bus overhead

**Disadvantages**:
- Tight coupling between producers and logging sinks
- Adding new consumers requires modifying every producer
- Difficult to add conditional routing (e.g., security events to separate sink)
- Testing requires mocking all consumers
- Code duplication across producers

**Why Not Selected**: Violates loose coupling principle. Modification cascade makes system brittle.

### Option 2: Tokio mpsc Channel Per Consumer

**Description**: Create a dedicated mpsc (multi-producer, single-consumer) channel for each consumer.

**Implementation**:
```rust
struct EventChannels {
    audit_tx: mpsc::Sender<MemoryEvent>,
    metrics_tx: mpsc::Sender<MemoryEvent>,
    webhook_tx: mpsc::Sender<MemoryEvent>,
}

impl CaptureService {
    pub fn capture(&self, memory: Memory) -> Result<MemoryId> {
        let id = self.storage.insert(&memory)?;
        let event = MemoryEvent::Captured { /* ... */ };

        // Must send to each channel explicitly
        self.channels.audit_tx.send(event.clone()).await?;
        self.channels.metrics_tx.send(event.clone()).await?;
        self.channels.webhook_tx.send(event).await?;

        Ok(id)
    }
}
```

**Advantages**:
- Dedicated backpressure per consumer
- Consumer-specific buffer sizes
- Type-safe channel per consumer

**Disadvantages**:
- Producers must know about all consumers (defeats decoupling)
- Adding a consumer requires modifying producers
- N sends per event (inefficient)
- Channel management complexity

**Why Not Selected**: Does not achieve decoupling. Producers still coupled to consumer list.

### Option 3: Tokio Broadcast Channel (Selected)

**Description**: Single broadcast channel where producers publish once and all subscribers receive.

**Implementation**:
```rust
pub struct EventBus {
    sender: broadcast::Sender<MemoryEvent>,
}

impl EventBus {
    pub fn publish(&self, event: MemoryEvent) {
        // Single send, all subscribers receive
        let _ = self.sender.send(event);
    }

    pub fn subscribe(&self) -> broadcast::Receiver<MemoryEvent> {
        self.sender.subscribe()
    }
}
```

**Advantages**:
- True decoupling: producers publish, consumers subscribe independently
- Single publish per event regardless of subscriber count
- Subscribers can join/leave dynamically
- Filtered subscriptions for efficiency
- Built into Tokio, battle-tested

**Disadvantages**:
- Broadcast buffer can overflow under load (lagged receivers)
- Requires Tokio async runtime
- Events must implement Clone
- No persistence (events lost if no subscribers)

**Risk Mitigation**:
- Buffer size configurable (default 1024, sufficient for typical load)
- Metrics for buffer utilization and lagged receivers
- Lagged receivers increment counter and continue (graceful degradation)

**Why Selected**: Best balance of decoupling, simplicity, and performance.

### Option 4: External Message Broker (Kafka, NATS, Redis Pub/Sub)

**Description**: Use an external message broker for event distribution.

**Implementation**:
```rust
impl CaptureService {
    pub async fn capture(&self, memory: Memory) -> Result<MemoryId> {
        let id = self.storage.insert(&memory)?;

        // Publish to external broker
        self.kafka_producer
            .send(
                FutureRecord::to("subcog-events")
                    .payload(&serde_json::to_vec(&event)?)
            )
            .await?;

        Ok(id)
    }
}
```

**Advantages**:
- Persistent event storage
- Cross-process/cross-host event distribution
- Replay capability
- Mature ecosystem and tooling

**Disadvantages**:
- Massive operational complexity for single-process use case
- Network latency for every event
- External dependency (availability concerns)
- Deployment complexity (must run broker)
- Overkill for Subcog's requirements

**Why Not Selected**: Subcog is a single-binary, single-process application. External broker adds unacceptable operational complexity for no benefit in this deployment model.

### Option 5: Trait-Based Observer Pattern

**Description**: Define an `EventObserver` trait and maintain a list of observers.

**Implementation**:
```rust
trait EventObserver: Send + Sync {
    fn on_event(&self, event: &MemoryEvent);
}

struct EventDispatcher {
    observers: Vec<Arc<dyn EventObserver>>,
}

impl EventDispatcher {
    fn dispatch(&self, event: MemoryEvent) {
        for observer in &self.observers {
            observer.on_event(&event);
        }
    }
}
```

**Advantages**:
- No async runtime dependency
- Type-safe observer registration
- Synchronous dispatch option

**Disadvantages**:
- Synchronous dispatch blocks producer
- Observer list management complexity
- No built-in filtering or buffering
- Re-inventing what broadcast channel provides

**Why Not Selected**: Broadcast channel provides superior semantics with less code.

## Decision

**Selected Option**: Tokio Broadcast Channel (Option 3)

### Rationale

1. **True Decoupling Achieved**: Producers publish to the event bus without knowledge of subscribers. The `CaptureService` does not import, reference, or depend on `AuditLogger`, `MetricsCollector`, or `WebhookDispatcher`.

2. **Dynamic Subscription**: Consumers can subscribe and unsubscribe at runtime without affecting producers or other consumers. This enables:
   - Hot-reloading of webhook configurations
   - Conditional metrics collection
   - Debug subscribers during troubleshooting

3. **Efficient Fan-Out**: Single `send()` call delivers to all subscribers. Unlike mpsc-per-consumer, there is no N-way duplication at the sender.

4. **Built-In Backpressure**: The bounded buffer (1024 events default) provides natural backpressure. Slow consumers receive `Lagged` errors and can track missed events.

5. **Tokio Integration**: Subcog already uses Tokio for async I/O (MCP server, HTTP hooks). Broadcast channel integrates seamlessly with existing runtime.

6. **Minimal Overhead**: In-process channel with no serialization, no network, no external dependencies. Event delivery is effectively free (microseconds).

### Implementation Architecture

#### Event Types

The `MemoryEvent` enum captures all lifecycle events with rich metadata:

```rust
/// Events emitted during memory operations.
#[derive(Debug, Clone)]
pub enum MemoryEvent {
    /// A memory was captured.
    Captured {
        meta: EventMeta,
        memory_id: MemoryId,
        namespace: Namespace,
        domain: Domain,
        content_length: usize,
    },
    /// A memory was retrieved via search.
    Retrieved {
        meta: EventMeta,
        memory_id: MemoryId,
        query: Arc<str>,  // Arc for zero-copy sharing
        score: f32,
    },
    /// A memory was updated.
    Updated {
        meta: EventMeta,
        memory_id: MemoryId,
        modified_fields: Vec<String>,
    },
    // ... Archived, Deleted, Redacted, Synced, Consolidated ...

    /// MCP server started.
    McpStarted {
        meta: EventMeta,
        transport: String,
        port: Option<u16>,
    },
    /// MCP tool execution completed.
    McpToolExecuted {
        meta: EventMeta,
        tool_name: String,
        status: String,
        duration_ms: u64,
        error: Option<String>,
    },
    /// Hook invocation event.
    HookInvoked {
        meta: EventMeta,
        hook: String,
    },
    // ... additional hook and MCP events ...
}
```

#### Event Metadata

Every event carries standardized metadata for observability:

```rust
/// Shared event metadata required for observability.
#[derive(Debug, Clone)]
pub struct EventMeta {
    /// Unique identifier for this event (UUID v4).
    pub event_id: String,
    /// Optional correlation identifier for request/trace linking.
    pub correlation_id: Option<String>,
    /// Event source component (e.g., "capture", "recall", "mcp").
    pub source: &'static str,
    /// Timestamp (Unix epoch seconds).
    pub timestamp: u64,
}
```

The `correlation_id` enables distributed tracing across event chains. When a capture triggers indexing which triggers embedding generation, all events share the same correlation ID.

#### Event Bus Implementation

```rust
const DEFAULT_EVENT_BUS_CAPACITY: usize = 1024;

/// Central event bus for broadcasting memory events.
#[derive(Clone)]
pub struct EventBus {
    sender: broadcast::Sender<MemoryEvent>,
}

impl EventBus {
    /// Creates a new event bus with the given buffer capacity.
    pub fn new(capacity: usize) -> Self {
        let (sender, _receiver) = broadcast::channel(capacity);
        Self { sender }
    }

    /// Publishes an event to all subscribers (best effort).
    ///
    /// Publication is fire-and-forget. If no subscribers exist or the
    /// buffer is full, the event is dropped and a metric is incremented.
    pub fn publish(&self, event: MemoryEvent) {
        metrics::counter!("event_bus_publish_total").increment(1);
        match self.sender.send(event) {
            Ok(_) => {
                metrics::gauge!("event_bus_queue_depth")
                    .set(self.sender.len() as f64);
            }
            Err(_) => {
                metrics::counter!("event_bus_publish_failed_total").increment(1);
            }
        }
    }

    /// Subscribes to the event bus.
    pub fn subscribe(&self) -> broadcast::Receiver<MemoryEvent> {
        metrics::counter!("event_bus_subscriptions_total").increment(1);
        self.sender.subscribe()
    }

    /// Subscribes with a predicate to filter events.
    pub fn subscribe_filtered<F>(&self, predicate: F) -> FilteredReceiver<F>
    where
        F: Fn(&MemoryEvent) -> bool,
    {
        FilteredReceiver {
            receiver: self.sender.subscribe(),
            predicate,
        }
    }

    /// Subscribes to events matching a specific event type.
    pub fn subscribe_event_type(
        &self,
        event_type: &'static str,
    ) -> FilteredReceiver<impl Fn(&MemoryEvent) -> bool> {
        self.subscribe_filtered(move |event| event.event_type() == event_type)
    }
}
```

#### Filtered Receiver

For consumers interested in specific event types:

```rust
/// Filtered receiver that yields events matching a predicate.
pub struct FilteredReceiver<F> {
    receiver: broadcast::Receiver<MemoryEvent>,
    predicate: F,
}

impl<F> FilteredReceiver<F>
where
    F: Fn(&MemoryEvent) -> bool,
{
    /// Receives the next event that matches the predicate.
    pub async fn recv(&mut self) -> Result<MemoryEvent, broadcast::error::RecvError> {
        loop {
            match self.receiver.recv().await {
                Ok(event) if (self.predicate)(&event) => return Ok(event),
                Ok(_) => {} // Skip non-matching events
                Err(broadcast::error::RecvError::Lagged(skipped)) => {
                    metrics::counter!("event_bus_lagged_total").increment(skipped);
                    // Continue receiving after lag
                }
                Err(err) => return Err(err),
            }
        }
    }
}
```

#### Global Event Bus

A singleton pattern provides convenient access:

```rust
static GLOBAL_EVENT_BUS: OnceLock<EventBus> = OnceLock::new();

/// Returns the global event bus, initializing it on first use.
pub fn global_event_bus() -> &'static EventBus {
    GLOBAL_EVENT_BUS.get_or_init(|| EventBus::new(DEFAULT_EVENT_BUS_CAPACITY))
}
```

### Consumer Examples

#### Audit Logger Subscription

```rust
impl AuditLogger {
    /// Starts the audit logger subscription to the event bus.
    pub fn start_subscription(&self, event_bus: &EventBus) {
        let logger = self.clone();
        let mut receiver = event_bus.subscribe();

        tokio::spawn(async move {
            loop {
                match receiver.recv().await {
                    Ok(event) => {
                        if let Err(e) = logger.log_event(&event).await {
                            tracing::error!(error = %e, "Failed to log audit event");
                        }
                    }
                    Err(broadcast::error::RecvError::Lagged(n)) => {
                        tracing::warn!(skipped = n, "Audit logger lagged behind event bus");
                    }
                    Err(broadcast::error::RecvError::Closed) => {
                        tracing::info!("Event bus closed, stopping audit subscription");
                        break;
                    }
                }
            }
        });
    }
}
```

#### Metrics Collector Subscription

```rust
fn start_metrics_subscription(event_bus: &EventBus) {
    let mut receiver = event_bus.subscribe();

    tokio::spawn(async move {
        while let Ok(event) = receiver.recv().await {
            match event {
                MemoryEvent::Captured { content_length, .. } => {
                    metrics::counter!("memory_captured_total").increment(1);
                    metrics::histogram!("memory_content_bytes").record(content_length as f64);
                }
                MemoryEvent::Retrieved { score, .. } => {
                    metrics::counter!("memory_retrieved_total").increment(1);
                    metrics::histogram!("memory_retrieval_score").record(score as f64);
                }
                MemoryEvent::McpToolExecuted { duration_ms, tool_name, .. } => {
                    metrics::histogram!("mcp_tool_duration_ms", "tool" => tool_name)
                        .record(duration_ms as f64);
                }
                _ => {}
            }
        }
    });
}
```

#### Webhook Dispatcher Subscription

```rust
impl WebhookDispatcher {
    pub fn start_subscription(&self, event_bus: &EventBus) {
        let dispatcher = self.clone();
        let mut receiver = event_bus.subscribe();

        tokio::spawn(async move {
            while let Ok(event) = receiver.recv().await {
                // Only dispatch for events that have webhook subscribers
                if dispatcher.has_subscribers_for(&event) {
                    if let Err(e) = dispatcher.dispatch(&event).await {
                        tracing::warn!(error = %e, "Webhook dispatch failed");
                    }
                }
            }
        });
    }
}
```

## Consequences

### Positive

1. **Complete Decoupling**: Producers (`CaptureService`, `RecallService`, `ConsolidationService`) have zero dependencies on consumers. They import only `EventBus` and `MemoryEvent`.

2. **Independent Consumer Evolution**: New consumers (e.g., analytics, replication) can be added without modifying any producer code. Consumers can be disabled, reconfigured, or replaced independently.

3. **Flexible Subscription Patterns**: Consumers can:
   - Subscribe to all events
   - Filter by event type
   - Filter by custom predicates
   - Sample events for high-volume scenarios

4. **Built-In Observability**: The event bus itself is instrumented:
   - `event_bus_publish_total`: Total events published
   - `event_bus_publish_failed_total`: Failed publications (buffer full)
   - `event_bus_queue_depth`: Current buffer utilization
   - `event_bus_subscriptions_total`: Subscription count
   - `event_bus_lagged_total`: Events missed by slow receivers

5. **Correlation ID Propagation**: Events carry correlation IDs that enable:
   - Distributed tracing across event chains
   - Log aggregation by request
   - Debugging complex flows

6. **Graceful Degradation**: If a consumer falls behind (lagged), it receives a `Lagged(n)` error indicating how many events were missed, then continues receiving. No crash, no data corruption.

7. **Zero External Dependencies**: No message broker to deploy, configure, or monitor. The event bus is a pure in-process construct.

### Negative

1. **Broadcast Buffer Overflow Risk**: Under extreme load (>1024 events in flight), slow consumers miss events. Mitigation:
   - Monitor `event_bus_lagged_total` metric
   - Alert when lag exceeds threshold
   - Increase buffer size for high-throughput deployments
   - Implement event batching for bulk operations

2. **Tokio Runtime Requirement**: The event bus requires a Tokio async runtime. This is not a significant constraint since Subcog already uses Tokio for MCP server and HTTP operations.

3. **No Event Persistence**: Events are ephemeral. If no subscriber exists when an event is published, it is lost. Mitigation:
   - Ensure subscribers start before producers
   - For critical events (audit), use synchronous fallback

4. **Clone Overhead**: Events must implement `Clone` for broadcast semantics. The `MemoryEvent` struct is designed for efficient cloning:
   - `Arc<str>` for query strings (pointer copy)
   - Small fixed-size fields (copy)
   - `Vec<String>` only for variable-length fields

5. **Debugging Event Chains**: Multi-hop event chains (capture -> index -> embed -> link) can be complex to trace. Mitigation:
   - Correlation IDs link all events in a chain
   - Structured logging with correlation ID in every handler
   - OpenTelemetry spans for visualization

### Neutral

1. **Single-Process Limitation**: The in-process event bus does not support cross-process or distributed event delivery. This matches Subcog's single-binary deployment model. If distributed deployment is needed in the future, the event bus interface can be adapted to use an external broker.

2. **Fire-and-Forget Semantics**: `publish()` does not return delivery confirmation. Producers cannot know if consumers received the event. This is intentional: producers should not wait for or depend on consumer behavior.

## Related Decisions

- **ADR-0004**: Event Bus for Cross-Component Communication - Original event bus design
- **ADR-0060**: Observability Expansion Priorities - Uses event bus for instrumentation
- **ADR-0037**: Model Selection - Events include embedding operations

## Implementation Notes

### Buffer Size Tuning

The default buffer size (1024) is chosen based on:
- Typical operation rate: ~100 operations/second
- Consumer processing time: ~1-10ms per event
- Headroom for burst traffic: 10x typical rate

For high-throughput deployments, adjust via environment variable:

```bash
export SUBCOG_EVENT_BUS_CAPACITY=4096
```

### Shutdown Handling

On graceful shutdown, the event bus sender is dropped, causing all receivers to receive `RecvError::Closed`. Consumers should handle this to complete in-flight work:

```rust
match receiver.recv().await {
    Err(broadcast::error::RecvError::Closed) => {
        // Flush any buffered events
        // Complete graceful shutdown
        break;
    }
    // ...
}
```

### Testing Strategy

The event bus enables clean testing:

```rust
#[tokio::test]
async fn test_capture_publishes_event() {
    let event_bus = EventBus::new(16);
    let mut receiver = event_bus.subscribe();

    let service = CaptureService::new(storage, event_bus.clone());
    let id = service.capture(memory).await?;

    let event = receiver.recv().await?;
    assert!(matches!(event, MemoryEvent::Captured { memory_id, .. } if memory_id == id));
}
```

## Links

- [Tokio Broadcast Channel Documentation](https://docs.rs/tokio/latest/tokio/sync/broadcast/)
- [Event-Driven Architecture Patterns](https://martinfowler.com/articles/201701-event-driven.html)
- [Observer Pattern](https://refactoring.guru/design-patterns/observer)

## More Information

- **Date:** 2026-01-04
- **Related:** ADR-0004 (`docs/adrs/adr_0004.md`)
- **Implementation:** `src/observability/event_bus.rs`, `src/security/audit.rs`, `src/models/events.rs`

## Audit

### 2026-01-15

**Status:** Pending

**Findings:** Initial audit pending.

**Summary:** ADR migrated to structured-madr format.

**Action Required:** Complete audit after implementation review.
